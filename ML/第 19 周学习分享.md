时间：2019年7月29日 ~ 2019年8月4日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
陈亮 | Principal Component Analysis | 1。这周主要学习了PCA(主成分分析)pca原理就是向方差最大的方向做线性投影, 主要是非监督学习里面进行数据降维和去相关。当我们数据维度不多时候我们可以不用考虑pca，但是有时候我们面对成千上万的数据维度，这样将会消耗很多计算资源。降低数据维度就意味部分信息会丢失，所以我们使用pca尽量减少信息的丢失。另外pca因为不用考虑参数，所以是一个很通用的工具，不会受到我们主观影响。2。找到了四篇pca相关的论文有空读一下。[1] Pearson, K. On Lines and Planes of Closest Fit to Systems of Points in Space. Philosophical Magazine. 2 (11): 559–572. 1901.[2] Ian T. Jolliffe. Principal Component Analysis. Springer Verlag, New York, 1986.[3] Scholkopf, B.,Smola,A.,Mulller,K.-P. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation, 10(5), 1299-1319, 1998.[4] Sebastian Mika,Bernhard Scholkopf,Alexander J Smola,Klausrobert Muller,Matthias Scholz Gun. Kernel PCA and de-noising in feature spaces. neural information processing systems, 1999. 3。这周还读到一篇不错的文章，一句话总结常用机器学习算法https://cloud.tencent.com/developer/article/1155290 有兴趣的同学可以看一看。


> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
