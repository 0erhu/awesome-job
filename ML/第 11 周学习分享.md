时间：2019年6月3日 ~ 2019年6月9日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
李小川 | 回顾 SVM 原理 | 在学习《机器学习实战》这本书中，在SVM的这一节中对于拉格朗日对偶性和KKT条件理解有些困难，之前在Ng的视频中，对此有所了解，但是现在看来还是没有理解。要是想理解SVM的背后原理，必须理解怎么求得的间隔最大的超平面。由原始的拉格朗日式子，怎么得到最优解。这一切都需要以理解拉格朗日对偶性为前提。故查阅资料对这个缺漏的知识点加以深刻理解。总结在 https://blog.csdn.net/lixc316/article/details/91040152
陈亮 | 处理数据集 + 复习逻辑回归 | 这周主要是利用 https://openpolicing.stanford.edu/ 这个数据集进行了一下练手，首先根据数据有没有缺失情况选择drop掉一些row。然后进行了一下数据类型的转换，为了方便之后对数据进行操作。之后进行了一下假设，用matplotlib，可视化验证了一下数据之间的是否具有关联性。另外发现其实如果我们处理的是multi-index series转换成dataframe貌似更好一点。之前一直被逻辑回归的名字困惑了好久，发现周志华老师的解释很好，logistic regression翻译成对数几率回归更恰当，之所以叫逻辑回归是有历史原因的David Cox 1958年的论文中最早出现了这个方法，而当时人们对于回归和分类的定义和今天不同，最后对比了一下线性回归和逻辑回归的区别和联系。
> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
