时间：2019年8月5日 ~ 2019年8月11日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
陈亮 | activation function + random forest | 1.深度学习是基于神经网络的，信号从神经元进入，经过activation function传入到下一层神经元，直到输出层，简单说activation function就是每层之间的映射方式。常见的几种activation function有sigmoid function, tanh function, ReLU function, leaky ReLU function, Exponential Linear Units function 目前大家使用比较频繁的是ReLU。2.随机森林是一个在Kaggle上很popular的算法。既可以做classification还可以做regression。很多人说之所以random forest在比赛中表现好是因为它不容易过拟合以及训练速度快。3.读论文时候发现对于machine learning中的很多数学基础实际上还不是很理解，俗话说的好搞机器学习只会调包不懂数学原理的都是在耍流氓。所以entrol了一个课程Mathematics for Machine Learning Specialization. 4.一点小感悟：最近发现其实在学习机器学习过程中，我们经常会陷入无休止的尝试理解不同的算法，从而消磨了自己学习机器学习的激情，如果让我重新制定机器学习的计划我觉得，首先目标一定要是会调包，https://scikit-learn.org/stable/tutorial/machine_learning_map/ 找到一些类似于这样的cheat sheet然后利用kaggle的一些小数据集不断验证各种算法的使用场景，然后再进入理论细节进行学习。简单说就是先做调包侠再花时间巩固理论基础。因为大部分人的精力和耐心是有限，很多人都是被自己不正确的学习方式劝退的。
feiwofeifeixiaowo | FM | 本周系统的读了[FM论文](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)， 之前对FM模型的了解都是通过博客文章，总是觉得差了那么点意思。推荐大家了解某个算法的时候结合着论文一起开展。

> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下
