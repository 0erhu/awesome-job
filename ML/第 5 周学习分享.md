学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
李小川 | 神经网络 | 这一节对神经网络进行了详细的学习，个人感觉是理解了神经网络的正向反向传播算法的运作机理。并且将公式推导了一遍，同时我参考了其他的资料，如B站Deeplearning的视频资料，2015年的一份BP算法详推PDF。对于课后作业的复现还是差劲了点。我把对这节内容所学具体的东西总结在我的博客。https://blog.csdn.net/lixc316/article/details/89417225
袁林|支持向量机，聚类|这一星期对Ng的这两张作业复现，代码的原理还是对公式的转化，这里总结了几个函数用法：np.round()将数据四舍五入， 在python3下，前一位为奇数，则进位，前一位为偶数，则舍去。也就是 “四舍六入五成双”，又叫 “四舍六入五凑偶”。np.unique() 删重排序, 保留数组中不同的值，返回两个参数，支持向量机中fit（）的用法，fit() 方法：用于训练SVM，具体参数已经在定义SVC对象的时候给出了，这时候只需要给出数据集X和X对应的标签y即可。
陈亮|量子机器学习|这一星期抽空听了一下量子力学入门感觉波尔是把量子力学变成玄学最大的功臣。另外也关注了一下quantum information processing和machine learning的交叉学科量子机器学习。目前我们可以通过一些simulator进行相关的实验。量子计算的强大主要是利用了量子纠缠(量子叠加态的必然结果)，所以决定它具备多么强大的运算能力，主要是取决于我们可以让多少个量子处于纠缠状态。大部分人都支持量子机器学习会给图像相关领域带来很大的改变。这里有一个很好的github资源可以关注一下：https://github.com/krishnakumarsekar/awesome-quantum-machine-learning
陆开胜 | 决策树/retinanet | 这周学习了决策树这块的内容，对于决策树主要就是划分特征的选择、决策树的生成以及剪枝，决策树不难，缺点也很突出，很容易产生过拟合，但是在此基础上的RF可以解决过拟合问题，面试常客GBDT也是在此基础上发展的。上周使用yolov3对小目标检测效果不太好，有人推荐使用retinanet试试，所以本周学习了这个目标检测算法，主要学习其中的focal-loss，也了解到one-stage精度不行的原因之一就是早期产生大量的bbox中前景和背景的类别很不均衡，focal loss的提出就是为了解决这个问题。目前已经将数据集制作好，接下来准备尝试看看下效果，希望能够找到对小目标检测效果好的算法。
成文浩 | One-shot Learning(2017-2018) | one-shot learning产生的动机主要是因为，现在主要用large-scale方法处理数据，但真实情况下，大部分类别没有数据积累，large-scale方法不完全适用。所以希望在学习了一定类别的大量数据后，对于新的类别，只需要少量的样本就能快速学习。存在的问题一方面是知识缺失，另一方面是需要大量的训练样本。one-shot learning的研究主要分为两类：第一类方法是直接基于有监督学习的方法，这是指没有其他的数据源，不将其作为一个迁移学习的问题看待，只利用这些小样本，在现有信息上训练模型，然后做分类；第二个是基于迁移学习的方法，是指有其他数据源时，利用这些辅助数据集去做迁移学习。
刘嘉艺 | 优化单目图像深度估计 | 对之前的loss做了调整，在之前的图像空域loss基础上加上了高通滤波后的频域的分量，目的在于强化网络对图像细节的表达能力。
刘天宇 | Fast-RCNN | 由于之前做的数据是时间序列，但是有个项目需要深度学习来处理图片数据，主要做图片识别，所以了解和研究了一下图像分割算法Fast-RCNN。明白了其基本框架和原理。正在阅读代码实现。
晓林 | 逻辑回归 多项式回归 | 在做一道课后题目的时候，突然有些陌生，细想之后就茅塞顿开了，大意是根据学生两门课的成绩和是否入学的数据，预测能否顺利入学。在matlab中，先用plotData.m实现可视化，完成sigmoid函数，重点在于说利用fminunc函数求最小代价时，costFunction函数可以处理xy已知数据，这样θ可获解。
> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
