时间：2019年7月8日 ~ 2019年7月14日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
七少 | 学习 Pytorch | 根据龙良曲的讲解，整理了自己的jupyter notebook。 https://github.com/xiaoxuebajie/pytorch_learning 
sampras | 数据可视化| 使用requests,sb4爬取Bilibili全职高手弹幕数据存储到数据库；保存了数据之后，对数据进行处理，在用jieba,pyechart进行可视化,12集中，前两集弹幕数量最多，从发弹幕长度来看，绝大部分用户只会发送2条以内的弹幕，通过词云图，大致可以看出用户的分布，但由于未采用多线程，只采用了一天的弹幕，词云图的效果较差
陈壮壮 | 生成对抗网络 | 整理了现有的生成网络在生成图片多样性不足，运行时间长，以及训练可能存在 模式坍塌 的情况，做了分析,目前正在尝试推导新的理论模型.
君君 | 深度学习基础理论及代码实现 | 1.理解反向传播中每一层节点处梯度的数学计算公式；2.利用numpy搭建简单的网络，包括权重初始化、激活函数、前向传播、误差计算、梯度计算、权重更新等过程，实现异或算法的回归拟合，最终利用保存的权重进行预测。跳出深度学习框架使用numpy实现深度学习，是一个理解深度学习的非常有效的方法，后续会继续学习用numpy实现深度学习更多相关算法。
唐洋 | 目标跟踪移植 | 将目标跟踪算法移植在树莓派上，代码都还好，最麻烦的是环境的安装，树莓派系统和linux还不一样，所以安装环境很麻烦。
安芯 | 最优化方法   | 最优化方法是一种数学方法，主要解决约束情况下寻找因素变化的量，常见的最优化方法：梯度下降、牛顿法、拟牛顿法、共轭梯度法。1、梯度下降和牛顿法种常用的方法是Batch/Stochastic gradient descent主要是权衡全局最优和计算快慢来选择，为了解决梯度方法存在下降慢后来提出了动量的方法Momentum（SGD+Momentum）及加速梯度下降的Nesterov（Nesterov Accelerated Gradient）的NAG算法；2、牛顿法、拟牛顿法、与共轭梯度法；3、Adagrad算法；4、Adam算法。
> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
