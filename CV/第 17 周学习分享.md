时间：2019年7月15日 ~ 2019年7月21日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
李钊 | GoogLeNet | 这周因为出差学习时间异常的少，翻译了一半GoogLeNet，上周的Alexnet还没写完代码。GoogLeNet是一种图像分类算法，论文大概读完一遍了。论文比较棒，感觉不光是算法，论文中言词都让人感觉以另一个程度的。再说GoogLeNet的算法，完全跟无脑的alexnet不一样，是另一种算法，不是那种完全靠添加多个卷积池化层来提升性能的。googlenet基本改变了以前那种固定的网络结构，改用更灵活的方式，将庞大的网络变的相对小巧的多。它做的贡献有两个，一个是使用1X1的卷积核进行升降维数，这样以最小计算成本实现。另一个是在多个尺寸上同时进行卷积再聚合,有一个filter concat模块，利用稀疏矩阵分解成密集矩阵计算的原理来加快收敛速度。总之，GoogLeNet的论文还没有翻译精读完成，里面涉及的一些论文也没去好好看，因此需要把这些全部看完才能完整的评价和学习这篇论文。
七少 | python数据包基础夯实 | 笔记地址：https://github.com/xiaoxuebajie/CV_learning
sampras | 实时计算梳理 | 1.shell,awk数据编程巩固        2.Flink实时流原理学习与实战      3.视频增强和超分辨率知识的梳理
君君 | 权重节点梯度的详细推导 | 1.针对最基本的全连接+激活函数这类型的网络结构中每一个节点处反向传播时梯度的计算以及权重增量，从而能够直观理解sigmoid当激活函数时为什么会发生梯度消失的状况；2.增加对卷积层和池化层中权重节点的梯度计算及相应权重增量的推导公式。PS: 详细推导公式见下方。
唐洋 | 基于树莓派的目标跟踪小车 | 算法移植成功，但是小车存在左右摇摆的情况，初步确定是树莓派处理速度达不到要求的原因。
陈壮 | 高斯过程定位| 1.高斯过程是一个非参模型，数据点越多模型参数越大，预测值服从一个分布，是一个概率模型，数据点离散，连续都可以,为了解决数据越来越多，模型参数越大的问题，提出了很多算法,下周会具体去看.
安芯 | 训练Nan问题分析 |训练深度模型时候出现Nan的原因和解决方法：这种情况个人总结是training sample中脏数据导致梯度计算的时候爆炸或者消失导致失去梯度没法训练。解决方法：（1）数据归一化主要是图像的尺寸要同一，各个类别要均衡及标签中尽量少含有空格，数据归一化的方法（减均值、除方差、加入Batch Normalization、L2 norm）;(2)更新参数初始化方法（Xavier、mars）;(3)减小学习率（batch_szie减小）；（4）加入gradient clipping；具体问题是bad data导致logits计算出现了0，0传递给log（x|x=0）——>无穷大即出现Nan，通过设置batch_size = 1，shuffle = False，一步步将sample定位到了所有可能的脏数据删掉。期间删了好几个依然会loss断崖为nan，一直定位一直删终于 work out!
之所以会这样是因为实际业务上的真实数据，有实际经验的就知道的现实的数据非常脏，基本上数据预处理占据我80%的精力。好怀念以前可以天真快乐的在open dataset上做task跑模型的时候，真是啥都不用管，专注模型算法。
> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下


### 君君的学习笔记

* 1.输出层权重梯度计算
  ![1](https://img-blog.csdnimg.cn/20190722123012578.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDU1NQ==,size_16,color_FFFFFF,t_70)
  
* 2.输出层前一层隐藏层权重的梯度计算
  ![2](https://img-blog.csdnimg.cn/20190722123526271.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDU1NQ==,size_16,color_FFFFFF,t_70)
  
* 3.其它隐藏层权重梯度及误差传播公式
  ![3](https://img-blog.csdnimg.cn/20190722123123183.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDU1NQ==,size_16,color_FFFFFF,t_70)

* 4.卷积层和池化层权重的残差回传
  ![4_1](https://img-blog.csdnimg.cn/2019072221144327.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDU1NQ==,size_16,color_FFFFFF,t_70)
  ![4_2](https://img-blog.csdnimg.cn/20190722211454880.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDU1NQ==,size_16,color_FFFFFF,t_70)
  
