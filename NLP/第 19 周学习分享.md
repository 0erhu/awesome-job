时间：2019年7月29日 ~ 2019年8月4日

| 学习人  | 学习任务                  | 学习心得和参考资料                                |
| ---- | --------------------- | ---------------------------------------- |
| 任星凯  | 2019法研杯比赛--阅读理解任务参赛总结 | [链接](https://zhuanlan.zhihu.com/p/76377422) |
| 段金强  | 学习xlnet               | xlnet：Generalized Autoregressive Pretraining for Language Understanding是Google提出的一个nlp模型，结果来看在长文本方面要比BERT高几个百分点。xlnet结合了AR模型（Autoregressive LM）的特点（自左向右来预测下一个词出现的概率）和AE模型（Autoencoder LM）的特点（BERT模型就是一个典型的AE语言模型，BERT随机mask掉一些单词，训练过程就是根据上下文对这些单词做预测最大化），提出了排列语言模型 (permutation language modeling objective)，简单来说，记住词的位置，然后随机打乱词的顺序，去预测不同片段内的不同词顺序的概率。有待深入，在家学习效率出奇的低。项目地址<https://github.com/zihangdai/xlnet> |
|刘洋|阿里云安全比赛|尝试不同的词向量，如word2vec和glove，但最佳的还是窗口长度为5的w2v，使用word2vec和glove的concat和average，但是结果不如word2vec。|
|李昌群|capsule网络|最大的创新在于动态路由机制与使用向量的模长来表征实体存在的概率，动态路由算法得到的连接权重即为胶囊全连接的权重。 整个训练过程包括有监督的BP以及无监督的动态路由算法|
|大鹏鹏|xgboost学习|学习了树模型的基本原理，Boosting算法的思想是将许多弱分类器集成在一起形成一个强分类器，XGBoost是一种提升树模型。在python环境下安装xgboost并且对参数设定有了简单了解|
|Xin|对新闻稿进行对话提取|这个内容预计会持续一个月的时间。1）判断说话状态，预计用到搜索树与动态规划；2）判断说话人与说话内容，预计用到NER与Dependency Parsing；3）更难的是如何判断说话内容的开始，还有结束，对于无引号内容，如果有多个完整带句号的句子组成说话内容，怎样区分，预计用到TF-IDF或PCA。|



> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
