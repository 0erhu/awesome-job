时间：2019年7月29日 ~ 2019年8月4日

| 学习人  | 学习任务                  | 学习心得和参考资料                                |
| ---- | --------------------- | ---------------------------------------- |
| 任星凯  | 2019法研杯比赛--阅读理解任务参赛总结 | [链接](https://zhuanlan.zhihu.com/p/76377422) |
| 段金强  | 学习xlnet               | xlnet：Generalized Autoregressive Pretraining for Language Understanding是Google提出的一个nlp模型，结果来看在长文本方面要比BERT高几个百分点。xlnet结合了AR模型（Autoregressive LM）的特点（自左向右来预测下一个词出现的概率）和AE模型（Autoencoder LM）的特点（BERT模型就是一个典型的AE语言模型，BERT随机mask掉一些单词，训练过程就是根据上下文对这些单词做预测最大化），提出了排列语言模型 (permutation language modeling objective)，简单来说，记住词的位置，然后随机打乱词的顺序，去预测不同片段内的不同词顺序的概率。有待深入，在家学习效率出奇的低。项目地址<https://github.com/zihangdai/xlnet> |
|刘洋|阿里云安全比赛|尝试不同的词向量，如word2vec和glove，但最佳的还是窗口长度为5的w2v，使用word2vec和glove的concat和average，但是结果不如word2vec。|
|李昌群|capsule网络|最大的创新在于动态路由机制与使用向量的模长来表征实体存在的概率，动态路由算法得到的连接权重即为胶囊全连接的权重。 整个训练过程包括有监督的BP以及无监督的动态路由算法|



> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
