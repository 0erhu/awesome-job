时间：2019年4月15日 ~ 2019年4月21日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------
段金强 | 学习 cbow流程 | 看了好多cbow资料，就属这个讲的过程最明白，可以直接跳到后面链接看文图结合。1、输入层：上下文单词的onehot. {假设单词向量空间dim为V，上下文单词个数为C}；2、隐藏层：所有onehot分别乘以共享的输入权重矩阵W. {V*N矩阵，N为自己设定的数，初始化权重矩阵W}，所得的向量 {因为是onehot所以为向量} 相加求平均作为隐层向量, size为1*N；3、乘以输出权重矩阵W' {N*V}，(W‘也为初始化，是为了保证最终得到的向量跟单词向量的维度一致）；4、得到向量 {1*V} 激活函数处理得到V-dim概率分布 {PS: 因为是onehot嘛，其中的每一维斗代表着一个单词}，概率最大的index所指示的单词为预测出的中间词（target word）；5、与true label的onehot做比较，误差越小越好；6、假设我们此时得到的概率分布已经达到了设定的迭代次数，那么现在我们训练出来的look up table应该为矩阵W。即，任何一个单词的one-hot表示乘以这个矩阵都将得到自己的word embedding。链接：https://www.zhihu.com/question/44832436
任星凯 | 学习FastQA模型，学习SMRCToolkit的使用 | 现在大多数的阅读理解系统都是 top-down 的形式构建的，也就是说一开始就提出了一个很复杂的结构（一般经典的就是 **emedding-, encoding-, interaction-, answer-layer** ），然后通过 ablation study，不断的减少一些模块配置来验证想法，大多数的创新点都在 interaction 层，比如BIDAF、R-Net等等，大量的工作都在问题和文章的交互query-aware表示上创新，类似人类做阅读理解问题的思路“重复多读文章”，“带着问题读文章”等等，普通的“阅读理解思路”也都被实现了，这篇论文作者发现了很多看似复杂的问题其实通过简单的 context/type matching heruistic 就可以解出来了，过程是选择满足条件的 answer spans:**1. 与 question 对应的 answer type 匹配，比如说问 when 就回答 time；2. 与重要的 question words 位置上临近；3.添加问题单词是否出现在文章中这一“重要”特征**，并没有使用复杂的question与context的交互，就取得了在SQuAD榜上与SOTA接近的结果，这篇论文之后，后来的研究者们在做MRC时也会将基础特征加入到embedding中进行共同训练，[开源链接](https://github.com/BrambleXu/keras_fastqa)。同时也推荐大家一个搜狗开源的机器阅读理解的工具：[SMRCToolkit](https://github.com/sogou/SMRCToolkit)，实现了BIDAF,BIDAF++,QANET,FusionNet,RNet,DrQA等多个MRC模型，同时也在SQuAD v1,SQuAD v2,COQA等数据集进行了测试。
李昌群 | 注意力机制与外部记忆 | 这一周准备了三门考试，没有读论文。阅读了《神经网络与深度学习-邱锡鹏》的第八章内容，这一节主要讲了注意力机制与外部记忆，读完这一章有一个清晰的认识。注意力机制本质是加权平均。注意力机制的计算可以分为两步：一是在所有输入信息上计算注意力分布，二是根据注意力分布来计算输入信息的加权平均。外部记忆为了增强网络容量，引入辅助记忆单元。 

> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。


