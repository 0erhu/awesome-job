时间：2019年4月15日 ~ 2019年4月21日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------
段金强 | 学习 cbow流程 | 看了好多cbow资料，就属这个讲的过程最明白，可以直接跳到后面链接看文图结合。1、输入层：上下文单词的onehot. {假设单词向量空间dim为V，上下文单词个数为C}；2、隐藏层：所有onehot分别乘以共享的输入权重矩阵W. {V*N矩阵，N为自己设定的数，初始化权重矩阵W}，所得的向量 {因为是onehot所以为向量} 相加求平均作为隐层向量, size为1*N；3、乘以输出权重矩阵W' {N*V}，(W‘也为初始化，是为了保证最终得到的向量跟单词向量的维度一致）；4、得到向量 {1*V} 激活函数处理得到V-dim概率分布 {PS: 因为是onehot嘛，其中的每一维斗代表着一个单词}，概率最大的index所指示的单词为预测出的中间词（target word）；5、与true label的onehot做比较，误差越小越好；6、假设我们此时得到的概率分布已经达到了设定的迭代次数，那么现在我们训练出来的look up table应该为矩阵W。即，任何一个单词的one-hot表示乘以这个矩阵都将得到自己的word embedding。链接：https://www.zhihu.com/question/44832436

> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。


