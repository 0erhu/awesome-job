| 学习人 | 学习任务                                | 学习心得                                                     | 参考资料                                                     |
| ------ | --------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 段金强 | 学习Transformer                         | 首先学习Transformer是由Encoders和Decoders两部分组成。Encoders:是由6个子encoder组成，Decoders同样也是。子encoder结构相同，都是由self-attention和 Feed Forward Neural Network组成。子decoder结构相同，都是由self-attention、Attention和 Feed Forward组成。流程：把每个单词的词向量传输到self-attention（它的作用通过上下文理解当前词，计算方式：为每个向量计算socre=q*k，然后归一化并使用softmax，结果乘以v，想相加之后得到最终结果Z矩阵），得到一个向量列表，再传给前馈神经网络。并由前馈神经网络传输到下一层encoder。再经过各个decoer层，最后由linear+softmax层得到最终结果。其中self-attention的改进方法，使用了multi-headed attention，multi-headed可以有8个self-attention组成，每一个attention都有一套qkv权值矩阵，所以会产生8个加权之后的特征矩阵，并压缩成一个矩阵。写的有点乱 | 参考资料：https://zhuanlan.zhihu.com/p/48508221，https://blog.csdn.net/qq_41664845/article/details/84969266。 |
| 任星凯 | 学习BIDAF源码                           | BIDAF可以视作对Match-LSTM匹配模型的改进。其主要变化在于：（1)词向量化层增加了对字的向量化；（2)在语义交互 充了问题中每个词在文档编码向量上的注意力分配向量， 以提升文档词-问题交互向量的语义交互程度；（3）改用双向LSTM网络二次语义编码。与单向LSTM相比，双向LSTM可以同时提取到每个词相关的上下文信息。其中的维度变化部分还未能完全看懂，后续需要继续学习。 | https://renxingkai.github.io/2019/03/21/bidaf/#以下是prepro-py文件的代码阅读与分析： |
| 李昌群 | Memory Network     使用记忆网络增强记忆 | 包括inference components和a long-term memory component 一个Memory Network由一个记忆数组m（一个向量的数组或者一个字符串数组，index by i）和四个组件（输入I，泛化G，输出O，回答R）组成。  简单来说，就是输入的文本经过Input模块编码成向量，然后将其作为Generalization模块的输入，该模块根据输入的向量对memory进行读写操作，即对记忆进行更新  比如论文中给出了一个G()函数的例子：  ![1555242408043](C:\Users\昌群\AppData\Local\Temp\1555242408043.png) 即将I(x)直接插入到记忆单元的插槽(slot)中，H(x)就是选择插槽的函数，这样只更新选择的插槽的记忆，而不更新其他的记忆。论文中也说，更复杂的G()函数，可以更新其他相关的旧的记忆，也可以在memory大小不够时进行遗忘。 然后Output模块就可以将输入和记忆单元联系起来，根据输入选择与之相关的记忆单元，将记忆按照与Question的相关程度进行组合得到输出向量。  最终Response模块根据输出向量编码生成一个自然语言的答案出来 文中给出了一个简单的R()函数，将输入和选择的记忆单元与此表中的每个单词进行评分Sr,然后选择得分最大的单词作为回答。 | 1. https://blog.csdn.net/irving_zhang/article/details/79094416  2.   https://blog.csdn.net/liuchonge/article/details/78082761 |

