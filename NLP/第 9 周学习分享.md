时间：2019年5月20日 ~ 2019年5月26日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
任星凯 | 高等工程数学考试，BERT代码学习与使用 | 周三数学考试，周三之前主要还是复习数学；之后又看了一遍BERT论文，使用pytorch版本的BERT代码进行尝试，用于cail19阅读理解任务，[参考链接](https://evilpsycho.github.io/2019/05/16/Kaggle-Jigsaw-%E6%81%B6%E6%84%8F%E8%AF%84%E8%AE%BA%E8%AF%86%E5%88%AB-bert/)
海龙 | 主动式闲聊 | 最近在做主动式闲聊对话的工作，构建了话题图，是双向的。图中的每个话题都是细粒度的名词，例如“昆虫、蚊子、眼睛、翅膀、食物”，总共一万多个。每个话题和20个话题相关联。模型还是seq2seq模型，考虑利用mem2seq或者cvae的方式，来对话加以利用，引导对话的生成。阅读论文：Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems，这篇论文利用memory，从dialog history 或者 Knowledge base 中读取内容，作用在每个generation step上，生成指定意义的回复。
李昌群 | S-NET | S-NET的主要不同在于进行了篇章排序和用生成模型来生成答案。<br />Passage ranking<br />从词级别到篇章级别来匹配问题和文章，首先使用问题r_Q来与篇章的每一个词进行attention计算，获得新的文章表示r_P，也就是使用问题与文章中的单词做attention，将与问题相关的词汇的权重提升，做加和后是最终的文章表示r_P。然后将问题的表示r_Q与篇章的表示r_P相连，过一个全连接层来获得一个匹配得分。对于每一篇文章，获得一个文章匹配得分g_i。<br />Answer Synthesis，作者使用一个seq2seq model生成答案。参考论文 summarization generation (Zhou etal. 2017) 
王耿鑫 | 学习Transformer | 这周主要学习了一下Transformer。随后整理一下笔记再附上笔记链接和参考资料。
段金强 | 学习CTC算法   | 语音识别 CTC算法：CTC 全称 是Connectionist Temporal Classification，是一种改进的RNN模型。RNN模型可以用来对两个序列之间的关系进行建模。但是，传统的RNN，标注序列和输入的序列是一一对应的。语音识别中的序列建模问题不是这样：识别出的字符序列或者音素序列长度远小于输入的特征帧序列。CTC解决这一问题的方法是，在标注符号集中加一个空白符号blank，然后利用RNN进行标注，最后把blank符号和预测出的重复符号消除。算法步骤：1、对齐：CTC算法不要求输入和输出的严格对齐，但是为了计算概率大小，总结两者的一些对齐规律的必要的，因为输入大于输出，为了做到有效对齐，它们之间要插入一个空白符号使之一一对应。它不对应任何输入，最后会从输出中被删除。2、损失函数：CTC的对齐方式展示了一种利用时间步长概率推测输出概率的方法。如果CTC的目标是一对输入输出(X,Y)，那Y的路径的概率之和为：一个输入对应多个输出的连乘并求和的概率。用动态规划算法快速计算loss，其关键思路是如果两种路径用相同步长映射到同一输出，那它们就能被合并。3、推理：训练好模型后，我们就需要根据给定输入计算可能的输出。一种启发式方法是根据每个时间步长计算路径的可能性，它能计算最高概率，这种启发式方法对于大多数程序都运行良好，尤其是当概率集中于某条路径时。但是，由于没有考虑到单个输出可能有多个对齐的事实，它有时也会错过真正的高概率输出。对于这个问题，我们可以通过修改集束搜索来解决。基于有限的计算，集束搜索不一定能找到最优可能的Y，但它至少具有一个良好的性质，让我们可以在更多计算（更大的集束尺寸）和渐近更优解之间折衷。


> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
