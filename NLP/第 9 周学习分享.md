时间：2019年5月20日 ~ 2019年5月26日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
任星凯 | 高等工程数学考试，BERT代码学习与使用 | 周三数学考试，周三之前主要还是复习数学；之后又看了一遍BERT论文，使用pytorch版本的BERT代码进行尝试，用于cail19阅读理解任务，[参考链接](https://evilpsycho.github.io/2019/05/16/Kaggle-Jigsaw-%E6%81%B6%E6%84%8F%E8%AF%84%E8%AE%BA%E8%AF%86%E5%88%AB-bert/)
海龙 | 主动式闲聊 | 最近在做主动式闲聊对话的工作，构建了话题图，是双向的。图中的每个话题都是细粒度的名词，例如“昆虫、蚊子、眼睛、翅膀、食物”，总共一万多个。每个话题和20个话题相关联。模型还是seq2seq模型，考虑利用mem2seq或者cvae的方式，来对话加以利用，引导对话的生成。阅读论文：Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems，这篇论文利用memory，从dialog history 或者 Knowledge base 中读取内容，作用在每个generation step上，生成指定意义的回复。
李昌群 | S-NET | S-NET的主要不同在于进行了篇章排序和用生成模型来生成答案。<br />Passage ranking<br />从词级别到篇章级别来匹配问题和文章，首先使用问题r_Q来与篇章的每一个词进行attention计算，获得新的文章表示r_P，也就是使用问题与文章中的单词做attention，将与问题相关的词汇的权重提升，做加和后是最终的文章表示r_P。然后将问题的表示r_Q与篇章的表示r_P相连，过一个全连接层来获得一个匹配得分。对于每一篇文章，获得一个文章匹配得分g_i。<br />Answer Synthesis，作者使用一个seq2seq model生成答案。参考论文 summarization generation (Zhou etal. 2017) 
王耿鑫 | 学习Transformer | 这周主要学习了一下Transformer。随后整理一下笔记再附上笔记链接和参考资料。 


> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
