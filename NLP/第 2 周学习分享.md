时间：2019年4月1日 ~ 2019年4月7日

学习人 | 学习任务 | 学习心得 | 参考资料
-- | -- | -- | --
任星凯 |	学习CNN在阅读理解方面的应用 |	学习了苏神的博客"搜狗阅读理解竞赛"比赛总结，模型未采用RNN's结构，仅用CNN+Attention结构进行对Q和C的交互式建模，因此，训练速度比传统的RNN结构交互要快的多，为了弥补句子长度问题，CNN部分使用膨胀CNN来进行实现，可以对更长的句子进行卷积，该模型Attention主要用于取代简单的Pooling来完成对序列信息的整合，包括将问题的向量序列编码为一个总的问题向量，将材料的序列编码为一个总的材料向量，同时加入类似Attention is all you need论文中的位置信息，在输出部分不用softmax，而是对整个序列都用sigmoid，这样既允许了材料中没有答案，也允许答案在材料中多次出现，同时加入常用的QA人工特征，最终也采用了模型融合以提升模型的效果，苏神还是tql!!!个人认为在学习深度学习时需要深入了解模型每一层，为什么会这样设计、好处?多总结，多实践，从而自己在搭建模型时才能融汇贯通。最后附上链接https://kexue.fm/archives/5409?tdsourcetag=s_pcqq_aiomsg
段金强 |	学习BERT	| BERT模型实际上是一个语言编码器，把输入的句子或者段落转换为特征向量。两大亮点：1、双向的transformer，利用masked模型实现双向 2、提出两种预训练方法，masked模型和下一个句子预测方法；训练流程：1、输入表示：可以对单个句子或一对文本句子，输入嵌入为token embedding，segmentation embedding，position embedding的求和 2、masked语言模型：如cbow预测当前单词，不过只取了15%的样本（每个batchsize只有15%的词被遮盖的原因，是性能开销。双向编码器比单项编码器训练要慢）3、预测下一个句子：预训练一个二分类的模型，来学习句子之间的关系。预测下一个句子的方法对学习句子之间关系很有帮助。训练方法：正样本和负样本比例是1：1，50%的句子是正样本，随机选择50%的句子作为负样本。4、预训练阶段参数 5、微调：微调阶段根据不同任务使用不同网络模型。
王耿鑫	| 学习fastText	| 之前用fastText做过短文本分类，效果异常的好，f1值达到0.995，所以特地学习了一下。fastText 的结构其实和 CBOW 模型差不多，只不过前者的输入除了词的embedding，还有一些n-gram特征。其原理很简单，就是对这些输入的向量取平均，线性激活后，全连接到输出层，再用 softmax 归一化每个类别的概率。一开始很难相信这么简单的结构能取得如此好的分类效果，而且速度还超级快。经过分析和参考了一些资料，我认为分类效果好的原因有这么几个：1. 除了词的embedding，还使用了n-gram特征，这使得很多词内部的“字”信息能被捕捉到；2. 词向量本身就蕴含了这么一种规律：两个词的向量距离越近，含义越相似。因此，在对输入的向量取平均时，就是意味着将输入文档的词向量累加起来并归一化，这可以将一个文档映射成空间中的一个向量，所以用这个向量来区分不同的文档是合理的。当用了softmax后，就相当于对输出空间划定了一些决策边界，用来进行分类；3. 输入文本较短，所以效果好；要是长文本，效果会变差。因为长文本本身包含了太多的信息，词向量经过平均后，失去了区分度（可通过大数定律或样本均值的方差来理解）。
李昌群 |	自然语言处理中的Attention机制	| 原来的Encoder-Decoder机制问题：输入序列不论长短都会被编码成一个固定长度的向量表示，而解码则受限于该固定长度的向量表示。encoder只将最后一个输出递给了decoder， decoder就相当于对输入只知道大概意思，而无法得到更多输入的细节，比如输入的位置信息。一个潜在的问题是，采用编码器-解码器结构的神经网络模型需要将输入序列中的必要信息表示为一个固定长度的向量，而当输入序列很长时则难以保留全部的必要信息（因为太多），尤其是当输入序列的长度比训练数据集中的更长时；对齐问题：只给我递来最后一个输出，不好；但如果把每个step的输出都传给我，怎么对齐？（比如翻译问题），attention机制：打破了传统编码器-解码器结构在编解码时都依赖于内部一个固定长度向量的限制。针对attention的变体主要有两种方式：1.一种是在attention 向量的加权求和计算方式上进行创新，2.另一种是在attention score（匹配度或者叫权值）的计算方式上进行创新，有Soft attention、global attention、动态attention、local attention，特殊的attention：self attention、key-value attention、multi-head attention。参考：https://blog.csdn.net/hahajinbu/article/details/81940355，https://www.cnblogs.com/shixiangwan/p/7573589.html
张江勃 |	自然语言处理的不平衡数据问题 |	项目中做分类问题会很容易遇到数据不均衡问题，此问题让让人深感头痛啊，我们目前尝试了上采样 下采样 随机采样 以及各种方法 但是对于结果都没有太大的效果，对于多分类问题目前来看，没有特别好的处理方法（也有可能有我不知道的好方法），现在我们在尝试把多分类转换成多个二分类，然后对loss进行加权。效果有提升但是不是特别好 应该是数据量级太少吧。请问下大家有比较好的处理数据不均衡问题的方法吗？
夏鑫林 |	 |最近有一个比赛快接近尾声了，这几天都在忙比赛的事情，nlp的话进展有点缓慢，看到各位学习进度还挺快，希望比赛完了以后可以跟各位一起交流	
