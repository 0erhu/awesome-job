打卡人|学习任务|学习心得
任星凯|看QANet Tensorflow版本源码 |	QANet现在仍然在SQuAD1.1榜上位居第四，通过阅读DuReader数据集版本的QANet，加深了对模型,encoder.decoder,fuse等层的更深理解，编码中用到的focal_loss等部分，
从图像借鉴到nlp，很值得学习，但是该作者开源的attention层用了基础的dot attention，可以后期加以改进，同时对tensorflow框架也有了更深的体会(吐槽一句，还是不容易上手编写复杂模型)
张江勃	|最近看了许多关于NLP方面的知识	| 我接触NLP是从word2vec开始的，我刚开始接触的时候非常好奇，计算机是如何把文字转换成向量的，而向量难道在真的能够表示一个字或一个词又或者是一句话吗？我带着疑问进入了NLP世界，WordEnbedding的发展我个人感觉这几年特别的快，尤其是前一阵，GPT1.0出来没多久Bert大黑马直接杀出，正在如火如茶的看Bert呢，微软的MT-DNN又来抢头牌，刚要看看有什么奇特之处，GPT2.0直接技压群雄(有钱就是好）。从这些来看，都是特么有钱人玩的游戏，出来了这么多的新技术，大家从总体上看是如出一辙，个人感觉差异不大。希望后期能有更牛叉的技术，期待Bert2.0。哈哈哈，NLP是一个很大的领域，目前只停留在使用一些算法做分类，而没有做过其他方面的比如语义分析，关键词提取等等，接下来的时间我会继续深入，下周我可能会提一些问题，希望大佬解答，哈哈哈！
夏鑫林 |	pytorch |	由于之前学习过一些nlp的相关知识，但是没有怎么时间，因此找了一个比赛，结合理论实践，今天完成了算法框架部分
王耿鑫	| 短文本理解总结	| 本文主要复习并总结了王仲远所著的《短文本数据理解》第4章-基于概念化的短文本理解。https://www.zybuluo.com/Godsing/note/1442628
段金强	|实现NER命名实体识别 |	NER识别方法有很多种，机器学习最常见的是HMM，CRF；深度学习LSTM，RNN，BiLST等，现在最常见的是混合模式，即RNN+CRF，BiLSTM+CRF等。我调研的是BiLSTM+CRF，通过BiLSTM获取上下文的关键信息，并用CRF状态之间的概率形成约束条件，来实现标注问题。
关键计算，对每一个样本的打分，打分模型分为两部分，第一部分为样本对于每个类别的分数，第二部分转移分数。然后用softmax求最终的概率。
