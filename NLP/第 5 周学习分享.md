学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
段金强 | 学习 Attention 原理 | 前两次打卡分别是BERT模型学习和BERT中的transformer，这次是transformer中的Attention；transformer用到的attention分为**self-attention**和**multi-head attention**
一、**self-attention**:将输入单词转化为嵌入向量，根据嵌入向量得到q、k、v三个向量，为每个向量计算socre=q*k，然后使用softmax归一化，结果点乘v，相加之后得到最终结果Z矩阵
二、**multi-head attention**：是有多层self Attention实现，就是先让 Q，K，V 做一个线性的投影（分别乘上个矩阵），再做 Attention，这样重复多次，将结果拼接起来，得到一个“多头” Attention。目的：一方面，从直觉上多次 Attention 操作就能够捕获更多的信息；另一方面，先进行的投影操作能够把 Q、K、V 映射到不同空间，也许能够发现更多的特征。
三、位置编码：给位置1，2，3，4...n等编码（也用一个embedding表示）。然后在编码的时候可以使用正弦和余弦函数，使得位置编码具有周期性，并且有很好的表示相对位置的关系的特性（对于任意的偏移量k，PE[pos+k]可以由PE[pos]表示）
总结：encoder self-attention：使用multi-head attention，输入的Q、K、V都是一样的（input embedding and positional embedding）
encoder-decoder attention：使用multi-head attention，输入为encoder的输出和decoder的self-attention输出，其中encoder的self-attention作为key and value，decoder的self-attention作为query
decoder self-attention：使用masked multi-head attention，输入的Q、K、V都是一样的（output embedding and positional embedding）
> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
