学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
段金强 | 学习 Attention 原理 | 前两次打卡分别是BERT模型学习和BERT中的transformer，这次是transformer中的Attention；transformer用到的attention分为**self-attention**和**multi-head attention**。一、**self-attention**:将输入单词转化为嵌入向量，根据嵌入向量得到q、k、v三个向量，为每个向量计算socre=q*k，然后使用softmax归一化，结果点乘v，相加之后得到最终结果Z矩阵。二、**multi-head attention**：是有多层self Attention实现，就是先让 Q，K，V 做一个线性的投影（分别乘上个矩阵），再做 Attention，这样重复多次，将结果拼接起来，得到一个“多头” Attention。目的：一方面，从直觉上多次 Attention 操作就能够捕获更多的信息；另一方面，先进行的投影操作能够把 Q、K、V 映射到不同空间，也许能够发现更多的特征。三、位置编码：给位置1，2，3，4...n等编码（也用一个embedding表示）。然后在编码的时候可以使用正弦和余弦函数，使得位置编码具有周期性，并且有很好的表示相对位置的关系的特性（对于任意的偏移量k，PE[pos+k]可以由PE[pos]表示）。总结：encoder self-attention：使用multi-head attention，输入的Q、K、V都是一样的（input embedding and positional embedding）；encoder-decoder attention：使用multi-head attention，输入为encoder的输出和decoder的self-attention输出，其中encoder的self-attention作为key and value，decoder的self-attention作为query；decoder self-attention：使用masked multi-head attention，输入的Q、K、V都是一样的（output embedding and positional embedding）
任星凯 | 看SMRC的BIDAF源代码 | 这周准备了两门考试，大部分时间在复习考试，也看了SMRC的BIDAF源代码，总结下常用MRC模型的一些tips:构建context与question的表示时，可以：使用word embeddings，英文可以加上char embeddings进行embedding层的常用套路;context中的词是否在问题中出现;要加上问题的类型,when,who,where,how,which等等embedding信息。得到以上表示以后，可以将他们concat起来，作为每个question与context的一个向量特征表示。最后可以使用MRT+最大似然估计作为损失函数进行训练。
夏鑫林 | 学习word2vec,elmo,bert | 1. 学习了词向量的发展历程，学习了elmo,bert应用到NER领域，并在源码层面上进行了学习 2. 这周面试了2家小公司，应该都拿到offer了  3. 查看了Facebook关于ctr的论文，以及 DCN对 浅层的设计，可以指定交叉幂数上限 
王耿鑫 | 学习HMM | 重新学习了一遍隐马尔可夫模型，这是一个常用来做序列标注的模型。<br />做了一下笔记，链接：https://www.zybuluo.com/Godsing/note/1465003 
李昌群 | 使用CNN进行垃圾邮件分类 | 了解使用CNN进行文本处理，与处理图像的不同在于filter的宽度应与embedding_dim相同，这样覆盖完整的一个词。并且选择多种不同的filter，可以学习到不同的单词组合（2个单词组合、三个单词组合、4个单词组合）。

张江勃| 微调Bert结构| 1.分享下学习心得，通过这次微调感觉到了 自己对TensorFlow 一无所知 ，很多的函数都不知道是干什么的，最后通过阅读源码 学到了很多，我修改是把分类的run_classifier.py 文件进行了修改，把y改成了TensorFlow的feature作为输入，删除了 原本DataProcessor里面的get_labels()方法，因为我们输入的y原本就是数值类型 所以不需要进行转换。修改了原本bert每一个sep 训练一个分类器 改为 训练 122个（这个是自己定的，因为我们需要所以修改为了122），中间进行 了N次调整，最后成功的运行了，只是不知道最后的结果如何，类似于 把bert的单任务改为多任务学习。哈哈哈。

> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
