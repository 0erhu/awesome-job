学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
段金强 | 学习 Attention 原理 | 前两次打卡分别是BERT模型学习和BERT中的transformer，这次是transformer中的Attention；transformer用到的attention分为**self-attention**和**multi-head attention**。一、**self-attention**:将输入单词转化为嵌入向量，根据嵌入向量得到q、k、v三个向量，为每个向量计算socre=q*k，然后使用softmax归一化，结果点乘v，相加之后得到最终结果Z矩阵。二、**multi-head attention**：是有多层self Attention实现，就是先让 Q，K，V 做一个线性的投影（分别乘上个矩阵），再做 Attention，这样重复多次，将结果拼接起来，得到一个“多头” Attention。目的：一方面，从直觉上多次 Attention 操作就能够捕获更多的信息；另一方面，先进行的投影操作能够把 Q、K、V 映射到不同空间，也许能够发现更多的特征。三、位置编码：给位置1，2，3，4...n等编码（也用一个embedding表示）。然后在编码的时候可以使用正弦和余弦函数，使得位置编码具有周期性，并且有很好的表示相对位置的关系的特性（对于任意的偏移量k，PE[pos+k]可以由PE[pos]表示）。总结：encoder self-attention：使用multi-head attention，输入的Q、K、V都是一样的（input embedding and positional embedding）；encoder-decoder attention：使用multi-head attention，输入为encoder的输出和decoder的self-attention输出，其中encoder的self-attention作为key and value，decoder的self-attention作为query；decoder self-attention：使用masked multi-head attention，输入的Q、K、V都是一样的（output embedding and positional embedding）
任星凯 | 看SMRC的BIDAF源代码 | 这周准备了两门考试，大部分时间在复习考试，也看了SMRC的BIDAF源代码，总结下常用MRC模型的一些tips:构建context与question的表示时，可以：使用word embeddings，英文可以加上char embeddings进行embedding层的常用套路;context中的词是否在问题中出现;要加上问题的类型,when,who,where,how,which等等embedding信息。得到以上表示以后，可以将他们concat起来，作为每个question与context的一个向量特征表示。最后可以使用MRT+最大似然估计作为损失函数进行训练。


> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
