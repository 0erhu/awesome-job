时间：2019年4月8日 ~ 2019年4月14日

学习人|学习任务|学习心得|参考资料
------ | ------ | ------ | -----
段金强|学习Transformer|首先学习Transformer是由Encoders和Decoders两部分组成。Encoders:是由6个子encoder组成，Decoders同样也是。子encoder结构相同，都是由self-attention和 Feed Forward Neural Network组成。子decoder结构相同，都是由self-attention、Attention和 Feed Forward组成。流程：把每个单词的词向量传输到self-attention（它的作用通过上下文理解当前词，计算方式：为每个向量计算socre=q*k，然后归一化并使用softmax，结果乘以v，想相加之后得到最终结果Z矩阵），得到一个向量列表，再传给前馈神经网络。并由前馈神经网络传输到下一层encoder。再经过各个decoer层，最后由linear+softmax层得到最终结果。其中self-attention的改进方法，使用了multi-headed attention，multi-headed可以有8个self-attention组成，每一个attention都有一套qkv权值矩阵，所以会产生8个加权之后的特征矩阵，并压缩成一个矩阵。写的有点乱|参考资料：https://zhuanlan.zhihu.com/p/48508221，https://blog.csdn.net/qq_41664845/article/details/84969266。
任星凯|学习BIDAF源码|BIDAF可以视作对Match-LSTM匹配模型的改进。其主要变化在于：（1)词向量化层增加了对字的向量化；（2)在语义交互 充了问题中每个词在文档编码向量上的注意力分配向量， 以提升文档词-问题交互向量的语义交互程度；（3）改用双向LSTM网络二次语义编码。与单向LSTM相比，双向LSTM可以同时提取到每个词相关的上下文信息。其中的维度变化部分还未能完全看懂，后续需要继续学习。|https://renxingkai.github.io/2019/03/21/bidaf/#以下是prepro-py文件的代码阅读与分析：
