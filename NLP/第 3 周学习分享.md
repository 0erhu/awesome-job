时间：2019年4月8日 ~ 2019年4月14日

学习人|学习任务|学习心得|参考资料
------ | ------ | ------ | -----
段金强|学习Transformer|首先学习Transformer是由Encoders和Decoders两部分组成。Encoders:是由6个子encoder组成，Decoders同样也是。子encoder结构相同，都是由self-attention和 Feed Forward Neural Network组成。子decoder结构相同，都是由self-attention、Attention和 Feed Forward组成。流程：把每个单词的词向量传输到self-attention（它的作用通过上下文理解当前词，计算方式：为每个向量计算socre=q*k，然后归一化并使用softmax，结果乘以v，想相加之后得到最终结果Z矩阵），得到一个向量列表，再传给前馈神经网络。并由前馈神经网络传输到下一层encoder。再经过各个decoer层，最后由linear+softmax层得到最终结果。其中self-attention的改进方法，使用了multi-headed attention，multi-headed可以有8个self-attention组成，每一个attention都有一套qkv权值矩阵，所以会产生8个加权之后的特征矩阵，并压缩成一个矩阵。写的有点乱|参考资料：https://zhuanlan.zhihu.com/p/48508221，https://blog.csdn.net/qq_41664845/article/details/84969266