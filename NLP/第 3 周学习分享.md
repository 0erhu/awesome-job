时间：2019年4月8日 ~ 2019年4月14日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
夏鑫林| 学习transformer | 1.前段时间的比赛已经结束，名次还行（10以内，这个比赛花的时间不多，所以已经达到预期了）2.这几天复习了下深度学习的基本知识包括吴恩达的深度学习视频 3.具体的查看了multi-head attention的实现代码，dense(2-d)attention的实现，以及 time（3d）的attention，对attention的实现有了代码实现层面的认识。另外transformer的代码也粗略的看了下，框架上能够看懂，但是如何使用到自己的项目还有些疑问.最后最近也找了个nlp的比赛来打算具体实践一下  预计下一周- 更多的看论文吧（fm,ffm,deepfm,xdeepfm等广告率预估的论文） - nlp的比赛上能够做出一个baseline，以及找到一个队友，还有算法 已经有些时间没刷了....得继续把算法捡起来。准备下周面试3家以上的公司找找感觉吧. https://github.com/bojone/attention https://zhuanlan.zhihu.com/p/54743941
段金强 | 学习Transformer                         | 首先学习Transformer是由Encoders和Decoders两部分组成。Encoders:是由6个子encoder组成，Decoders同样也是。子encoder结构相同，都是由self-attention和 Feed Forward Neural Network组成。子decoder结构相同，都是由self-attention、Attention和 Feed Forward组成。流程：把每个单词的词向量传输到self-attention（它的作用通过上下文理解当前词，计算方式：为每个向量计算socre=q*k，然后归一化并使用softmax，结果乘以v，想相加之后得到最终结果Z矩阵），得到一个向量列表，再传给前馈神经网络。并由前馈神经网络传输到下一层encoder。再经过各个decoer层，最后由linear+softmax层得到最终结果。其中self-attention的改进方法，使用了multi-headed attention，multi-headed可以有8个self-attention组成，每一个attention都有一套qkv权值矩阵，所以会产生8个加权之后的特征矩阵，并压缩成一个矩阵。写的有点乱。参考资料：https://zhuanlan.zhihu.com/p/48508221，https://blog.csdn.net/qq_41664845/article/details/84969266。 |
 任星凯 | 学习BIDAF源码                           | BIDAF可以视作对Match-LSTM匹配模型的改进。其主要变化在于：（1)词向量化层增加了对字的向量化；（2)在语义交互 充了问题中每个词在文档编码向量上的注意力分配向量， 以提升文档词-问题交互向量的语义交互程度；（3）改用双向LSTM网络二次语义编码。与单向LSTM相比，双向LSTM可以同时提取到每个词相关的上下文信息。其中的维度变化部分还未能完全看懂，后续需要继续学习。https://renxingkai.github.io/2019/03/21/bidaf/#以下是prepro-py文件的代码阅读与分析： 
 李昌群 | Memory Network     使用记忆网络增强记忆 | 包括inference components和a long-term memory component 一个Memory Network由一个记忆数组m（一个向量的数组或者一个字符串数组，index by i）和四个组件（输入I，泛化G，输出O，回答R）组成。  简单来说，就是输入的文本经过Input模块编码成向量，然后将其作为Generalization模块的输入，该模块根据输入的向量对memory进行读写操作，即对记忆进行更新  比如论文中给出了一个G()函数的例子， 即将I(x)直接插入到记忆单元的插槽(slot)中，H(x)就是选择插槽的函数，这样只更新选择的插槽的记忆，而不更新其他的记忆。论文中也说，更复杂的G()函数，可以更新其他相关的旧的记忆，也可以在memory大小不够时进行遗忘。 然后Output模块就可以将输入和记忆单元联系起来，根据输入选择与之相关的记忆单元，将记忆按照与Question的相关程度进行组合得到输出向量。  最终Response模块根据输出向量编码生成一个自然语言的答案出来 文中给出了一个简单的R()函数，将输入和选择的记忆单元与此表中的每个单词进行评分Sr,然后选择得分最大的单词作为回答。 参考资料：1. https://blog.csdn.net/irving_zhang/article/details/79094416  2.   https://blog.csdn.net/liuchonge/article/details/78082761 
 王耿鑫 | 继续学习fastText                        | 上周学习了fastText的原理并分析了用于分类效果好的原因，而 **fasttext 实际上也可以用来训练词向量，那么它和 Word2vec 有什么区别？**<br />fasttext 本质上是 word2vec 的扩展，最主要的区别是 fasttext 使用了字符级的 n-grams，对于中文而言，就是字级别。<br />比如说，对于一个词 "apple"，设 ngram 最小值和最大值都为 3，那么 fasttext 会把它拆分成 "<ap", "app", "ppl", "ple", "le>" 这么几个 n-gram 来进行训练，最后 "apple" 这个词的词向量由这几个 n-gram 求和得到。<br />好处：<br />1. 对于低频词，可以比 word2vec 得到更好的词向量。即使一个词出现频率低，它仍然可以共享其它词的 n-gram，从而得到的词向量仍然会比较好。<br />2. 甚至对于未登录词，仍然可以通过该词的 character n grams 组合得到它的词向量，即使这个词在训练语料中从未出现过。<br />但是从实践的角度来看，超参数（n-gram）的选择会显得至关重要，因为：<br />1. 训练时间更长：由于使用了字符级 n-gram，对于包含若干 n-gram 的词，在前向传播的时候需要计算这些 n-gram 的平均，在反向传播的时候，需要同时更新这些 n-gram 的向量，所以需要的训练时间会比 word2vec 长很多；<br />2. 消耗内存更大：因为使用了字符级 n-gram，n-grams 的数量会变得相当庞大，我们需要训练大量的 n-gram，这会及其消耗内存。 
