时间：2019年5月6日 ~ 2019年5月12日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
任星凯 | 做百度阅读理解比赛，看了一篇词向量对比的论文 | 众所周知，预训练好的词向量有不同的维度，比如预训练好的GloVe词向量有从50-300维等的词向量表示，但这些不同维度的表示有什么区别，以及在什么时候该用什么维度的词向量（虽然各论文中大家大多用了300维的词向量），这些问题我也确实不太清除。这篇论文解答了我的这些困惑，写的还是很精彩的。[论文原链接](https://arxiv.org/abs/1703.00993)这篇论文主要解决了两个问题点： **- 在阅读理解任务中应该使用什么样的预训练词向量** **- 测试阶段对于OOV词应怎样处理** **词向量对比：** - GloVe (50-300) - word2vec (300)  直接说结果吧：1. 词向量的对比 - 第一个结果：**使用在合适的语料库上训练的词向量可以比随机初始化提高3-6％。** 然而，用于预训练的语料库和方法是重要的选择：例如，在CBT上训练的word2vec词向量比随机词向量执行效果更差。 另请注意，**在每种情况下，GloVe词向量都优于在同一语料库中训练的word2vec嵌入。** 但是由于词向量对训练参数很敏感，并不能说GloVe一定比word2vec好，但确实从各个论文中也可以看出，一般会优先选择GloVe。 - 第二个结果：**对比GloVe不同维度对实验结果的影响(50-300)**：随着词向量维度的增加，实验结果性能是下降的。但是即使使用了300维的GloVe词向量，实验结果仍然比word2vec词向量效果好。 - 第三个结果：**在使用原始语料进行训练时，要先将停用词去掉**，与停用词的共现提供关于特定单词的语义的很少有意义的信息，因此具有高百分比的语料库可能不会产生高质量的向量。**训练词向量时，超参数调节很重要。** 2. 处理OOV词 - 第一个结果：**一般对于OOV词的处理都是赋予一个固定大小不变的词向量（UNK）**。这种方法忽略了这样一个事实，即分配为UNK的许多单词可能已经训练过VG中可用的词向量。实验中在测试时，任何新token将被分配其GloVe向量（如果存在）或UNK的向量。 - 第二个结果，**不是为所有OOV词分配一个共同的UNK向量，而是为它们分配未经训练但唯一的随机向量可能更好。此方法在训练时访问测试集词表是没必要的。** **总结** 作者已经证明，用于初始化单词向量的预训练词向量的选择对于阅读理解的神经网络模型的性能具有显著影响。在测试时处理OOV词的方法也是如此。**根据作者的实验，我们建议使用现成的GloVe词向量，并在测试时将预先训练的GloVe向量（如果可用）或随机但唯一的向量分配给OOV词。**
段金强 | fasttext使用心得 | fasttext原理网上有很多大家可以搜搜看，现在只说使用的一些体会；安装：现在fasttext包括好几个版本（fasttext-win，fasttext，fastText，pyfasttext），但基本都是安照Facebook中的来实现的，并且这几个版本使用的函数还太不一样。其中兼容最好的是[fastText](https://github.com/facebookresearch/fastText/archive/v0.2.0.zip#)这个版本，对linux或者windows都能很好地兼容，最好不要pip安装；训练：fasttext分类同样参数同样样本，训练的结果还不太一样，训练时给出的结果不一定准，在测试集上验证之后，最好抽些例子预测，人为判断更能检验出模型的好坏。fasttext训练很快，但是随着样本数量的增加，训练时间也逐步增加。当调整参数使模型到达一定程度，再调参数，效果也基本稳定了（感觉到了上限）。
李昌群 | Bert代码；   论文：Match-LSTM，BiDAF | 接上一周看了bert代码，主要看了modeling.py（bert主要流程是先embedding，包括位置和token_type的embedding，然后调用transformer得到输出结果，bert对于transformer的使用仅限于encoder，没有decoder的过程）和微调部分（着重看了看根据不同任务调整输入格式和对loss的构建）。学习阅读理解模型论文Match-LSTM和BiDAF。在interaction交互层区别在于BiDAF引入了双向注意力机制。Match-LSTM只有Passage对Question的注意力，Passage中每个单词关注Question中的哪些单词。BiDAF引入了Question看Passage的注意力在哪，这样可以计算得到Question眼中的Passage哪些单词重要哪些不重要. 


> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
