时间：2019年9月23日 ~ 2019年9月29日

| 学习人  | 学习任务      | 学习心得和参考资料                                |
| ---- | --------- | ---------------------------------------- |
| 段金强  | 学习 ALBERT | ALBERT 又叫 A LITE BERT，顾名思义就是一个轻量级的 BERT 模型，和BERT比三个变化点：1、嵌入向量参数化的因式分解不再将 one-hot 向量直接映射到大小为 H 的隐藏空间，而是先将它们映射到一个低维词嵌入空间 E，然后再映射到隐藏空间。通过这种分解，研究者可以将词嵌入参数从 O(V × H) 降低到 O(V × E + E × H)，这在 H 远远大于 E 的时候，参数量减少得非常明显，减少计算量，加快计算时间。2、跨层参数共享：所有层权重共享3、句间连贯性损失：句间建模使用基于语言连贯性的损失函数。对于 ALBERT，研究者使用了一个句子顺序预测（SOP）损失函数，它会避免预测主题，而只关注建模句子之间的连贯性。 |

> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
