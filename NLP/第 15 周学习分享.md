时间：2019年7月1日 ~ 2019年7月7日

| 学习人  | 学习任务      | 学习心得和参考资料                                |
| ---- | --------- | ---------------------------------------- |
| 段金强  | 学习 mass模型 | MASS（ Masked Sequence to Sequence Pre-training）：由微软亚洲研究院提出的新的预训练方法，专门针对序列到序列的自然语言生成任务，MASS对句子随机屏蔽一个长度为k的连续片段，然后通过编码器-注意力-解码器模型预测生成该片段。论文地址：https://arxiv.org/pdf/1905.02450.pdf。 MASS预训练有以下几大优势： （1）解码器端其它词（在编码器端未被屏蔽掉的词）都被屏蔽掉，以鼓励解码器从编码器端提取信息来帮助连续片段的预测，这样能促进编码器-注意力-解码器结构的联合训练； （2）为了给解码器提供更有用的信息，编码器被强制去抽取未被屏蔽掉词的语义，以提升编码器理解源序列文本的能力； （3）让解码器预测连续的序列片段，以提升解码器的语言建模能力。 MASS有一个重要的超参数k（屏蔽的连续片段长度），通过调整k的大小，MASS能包含BERT中的屏蔽语言模型训练方法以及GPT中标准的语言模型预训练方法，使MASS成为一个通用的预训练框架。（原文更详细：<http://www.elecfans.com/d/932765.html>） 感觉和其他预训练的核心思想差不多，都是挡住一部分，去预测，通过变换不同的模型，达到最好的效果。下周看下GPT模型。 |
| 刘洋  | 学习词向量 | 由于比赛成绩不是很理想，这周学习了一下Transformer和BERT,其中Transformer中使用的self-attention解决了RNN无法并行化的问题。而基于上下文的词向量，相比于word2vec的词向量，得到的更像是句子的对应向量。但是实践部分只是简单看了下tensorflow hub，还没有具体实践。|
| Yang | RNN、LSTM、GRU | 了解其中网络结果，各个门的细节。 |
|Xin|二元语言模型|学习了分别基于规则和基于概率来设计简单的语句生成器，并构建简单二元语言模型。学习了怎样进行文字筛选，和对所选内容进行分词。根据概率来判断生成的句子中哪个更加具有合理性。|
|李昌群|实现西部世界对话系统（简单）|本周参加国防科大暑期学校（还有考试），课余时间简单实现了西部世界对话系统（基于规则生成语言，基于n-gram来计算概率）|
|任星凯|学习DCMN模型|本周考了数字图像处理，学习了XLNET公布之前，多选阅读理解排行榜上SOTA的DCMN模型，[论文链接](https://www.researchgate.net/publication/330701031_Dual_Co-Matching_Network_for_Multi-choice_Reading_Comprehension)，该模型基于BERT进行设计，改进了BERT最后一层使用第一个词代表hidden state的情况，主要架构还是使用传统阅读理解模型的架构：首先，作者使用BERT作为编码层，分别得到段落，问题和答案选项的上下文表示。 然后构造匹配层以获得通过问题 - 答案三元组匹配表示，其对问题的位置信息和与该段落的特定上下文匹配的候选答案进行编码。 最后，作者对从字级到序列级的匹配表示应用分层聚合方法，然后从序列级到文档级应用。在XLNET公布之前达到了RACE数据集的SOTA|

> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
