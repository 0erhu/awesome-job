时间：2019年7月1日 ~ 2019年7月7日

| 学习人  | 学习任务      | 学习心得和参考资料                                |
| ---- | --------- | ---------------------------------------- |
| 段金强  | 学习 mass模型 | MASS（ Masked Sequence to Sequence Pre-training）：由微软亚洲研究院提出的新的预训练方法，专门针对序列到序列的自然语言生成任务，MASS对句子随机屏蔽一个长度为k的连续片段，然后通过编码器-注意力-解码器模型预测生成该片段。论文地址：https://arxiv.org/pdf/1905.02450.pdf。 MASS预训练有以下几大优势： （1）解码器端其它词（在编码器端未被屏蔽掉的词）都被屏蔽掉，以鼓励解码器从编码器端提取信息来帮助连续片段的预测，这样能促进编码器-注意力-解码器结构的联合训练； （2）为了给解码器提供更有用的信息，编码器被强制去抽取未被屏蔽掉词的语义，以提升编码器理解源序列文本的能力； （3）让解码器预测连续的序列片段，以提升解码器的语言建模能力。 MASS有一个重要的超参数k（屏蔽的连续片段长度），通过调整k的大小，MASS能包含BERT中的屏蔽语言模型训练方法以及GPT中标准的语言模型预训练方法，使MASS成为一个通用的预训练框架。（原文更详细：<http://www.elecfans.com/d/932765.html>） 感觉和其他预训练的核心思想差不多，都是挡住一部分，去预测，通过变换不同的模型，达到最好的效果。下周看下GPT模型。 |
| 刘洋  | 学习词向量 | 由于比赛成绩不是很理想，这周学习了一下Transformer和BERT,其中Transformer中使用的self-attention解决了RNN无法并行化的问题。而基于上下文的词向量，相比于word2vec的词向量，得到的更像是句子的对应向量。但是实践部分只是简单看了下tensorflow hub，还没有具体实践。|

> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
