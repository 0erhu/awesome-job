时间：2019年7月1日 ~ 2019年7月7日

| 学习人  | 学习任务      | 学习心得和参考资料                                |
| ---- | --------- | ---------------------------------------- |
| 段金强  | 学习 mass模型 | MASS（ Masked Sequence to Sequence Pre-training）：由微软亚洲研究院提出的新的预训练方法，专门针对序列到序列的自然语言生成任务，MASS对句子随机屏蔽一个长度为k的连续片段，然后通过编码器-注意力-解码器模型预测生成该片段。论文地址：https://arxiv.org/pdf/1905.02450.pdf。 MASS预训练有以下几大优势： （1）解码器端其它词（在编码器端未被屏蔽掉的词）都被屏蔽掉，以鼓励解码器从编码器端提取信息来帮助连续片段的预测，这样能促进编码器-注意力-解码器结构的联合训练； （2）为了给解码器提供更有用的信息，编码器被强制去抽取未被屏蔽掉词的语义，以提升编码器理解源序列文本的能力； （3）让解码器预测连续的序列片段，以提升解码器的语言建模能力。 MASS有一个重要的超参数k（屏蔽的连续片段长度），通过调整k的大小，MASS能包含BERT中的屏蔽语言模型训练方法以及GPT中标准的语言模型预训练方法，使MASS成为一个通用的预训练框架。（原文更详细：<http://www.elecfans.com/d/932765.html>） 感觉和其他预训练的核心思想差不多，都是挡住一部分，去预测，通过变换不同的模型，达到最好的效果。下周看下GPT模型。 |
| 刘洋  | 学习词向量 | 由于比赛成绩不是很理想，这周学习了一下Transformer和BERT,其中Transformer中使用的self-attention解决了RNN无法并行化的问题。而基于上下文的词向量，相比于word2vec的词向量，得到的更像是句子的对应向量。但是实践部分只是简单看了下tensorflow hub，还没有具体实践。|
| Yang | RNN、LSTM、GRU | 了解其中网络结果，各个门的细节。 |
|Xin|二元语言模型|学习了分别基于规则和基于概率来设计简单的语句生成器，并构建简单二元语言模型。学习了怎样进行文字筛选，和对所选内容进行分词。根据概率来判断生成的句子中哪个更加具有合理性。|
|李昌群|实现西部世界对话系统（简单）|本周参加国防科大暑期学校（还有考试），课余时间简单实现了西部世界对话系统（基于规则生成语言，基于n-gram来计算概率）|
|任星凯|学习DCMN模型|本周考了数字图像处理，学习了XLNET公布之前，多选阅读理解排行榜上SOTA的DCMN模型，[论文链接](https://www.researchgate.net/publication/330701031_Dual_Co-Matching_Network_for_Multi-choice_Reading_Comprehension)，该模型基于BERT进行设计，改进了BERT最后一层使用第一个词代表hidden state的情况，主要架构还是使用传统阅读理解模型的架构：首先，作者使用BERT作为编码层，分别得到段落，问题和答案选项的上下文表示。 然后构造匹配层以获得通过问题 - 答案三元组匹配表示，其对问题的位置信息和与该段落的特定上下文匹配的候选答案进行编码。 最后，作者对从字级到序列级的匹配表示应用分层聚合方法，然后从序列级到文档级应用。在XLNET公布之前达到了RACE数据集的SOTA|
|陈俊富|学习问答，以及tensorflow框架|看了一篇知识图谱问答的论文，该文提出了一种新的区分概念和实例的知识图谱表示方法，将上下位关系与普通的关系做了区分，可以很好地解决上下位关系的传递性问题，并且能够表示概念在空间中的层析与包含关系。它的贡献有三点，1.第一次提出并形式化了知识图谱嵌入过程中概念与实例区分。2.提出了一个新的嵌入模型TransC模型，该模型区分了概念与实例，并能处理isA关系的传递性。3.基于YAGO新建了一个用于评估的数据集。论文地址：https://arxiv.org/abs/1811.04588|
| 王耿鑫 | 了解LightGBM | 由于之前学习过 XGBoost，现在遇到 LightGBM，自然而然会和前者进行对比。总的来说，有这么几点区别：<br/>1. LightGBM 采用的是 leaf-wise 决策树生长策略，而 XGBoost 是 level-wise，即逐层对所有叶子进行分裂。由于 leaf-wise 每次选择当前所有叶子节点中分裂增益最大的节点进行分裂，递归进行，所以能对数据拟合得更加精细和直接，训练速度也更快，但缺点也很明显，容易过拟合，但这可以通过限制树的深度来抑制。<br/>2. LightGBM 采用了直方图算法，对特征进行分桶，只保留离散化之后的值，这样的好处是可以减少内存的消耗，同时加速了计算，但缺点是对数据拟合得相对粗糙一点（不过基本可以忽略）。虽然 XGBoost 也实现了近似直方图算法，但效率上仍然比 LightGBM 差不少。<br/>3. LightGBM 还支持 Pandas categorical 类型的特征。在特征分裂时，每个取值都被当做一个桶，分裂增益计算的是“是否属于某个类别值”的增益，所以实际效果相当于 one-hot 编码。<br/>4. 还有 并行化 以及 网络通信优化 方面的内容，但是没太看懂o(╯□╰)o，先不展开了 |

> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
