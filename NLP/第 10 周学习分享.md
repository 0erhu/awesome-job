时间：2019年5月27日 ~ 2019年6月2日

| 学习人  | 学习任务        | 学习心得和参考资料                                |
| ---- | ----------- | ---------------------------------------- |
| 段金强  | 学习WaveNet网络 | WaveNet可以用来做语音识别或语音生成。 WaveNet是2016年主要由Google旗下的Deepmind团队推出，提到借鉴了PixelCNN在图像上的应用，大致的思想就是利用图像中先前生成的像素点来进行新像素点的生成。图像是二维的，那么应用在音频信号中，则是一维的。基于这种理念，我们可以想到生成我们当前的音频信号也可以基于先前的音频。WaveNet设计思路，使用一个叠层的一维卷积（也称为因果卷积casual convolutional layers）来进行表示。 如果要提高我们的感受野，那么我们需要增加额外的层数，最终导致的结果是，我们需要花费巨额的计算量。 为了减少计算量和提高感受野，论文中使用了膨胀因果卷积（dilated casual convolutional layer）。 方法：每个卷积层都对前一层进行卷积，卷积核越大，层数越多，时域上的感知能力越强，感知范围越大。在生成过程中，每生成一个点，把该点放到输入层最后一个点继续迭代生成即可。 |
| 任星凯  | 学习Bert在阅读理解上的用法 | 使用的是pytorch版本的bert，把源码大致过了一遍，pytorch版本的代码看起来确实比TF版本更舒服点，可读性更高，这周实现了在Bert embedding层上concat其他文本特征(wiq、问题类型等)，但是效果都变差了，可能是自己打开方式不对；在Bert后面加了对答案的分类判别；只打开最后两层，冻结之前的层等思路 |
| 李昌群 | Elmo | ELMo是一种在词向量（vector）或词嵌入（embedding）中表示词汇的新方法。ELMo中每个词对应的向量实际上是一个包含该词的整个句子的函数<br />ELMo的词向量是在双层双向语言模型（two-layer bidirectional language model , biLM）上计算的。这种模型由两层叠在一起，每层都有前向（forward pass）和后向（backward pass）两种迭代。<br />原始词向量是使用字符级卷积神经网络将文本中的词进行转换的 |
| 王耿鑫 | 事件抽取模板挖掘 | 之前在公司实习时，做了一段时间事件抽取模板的挖掘，简单总结一下：<br />目前，事件抽取常用的方法可以分为两大类：基于模板匹配的方法和基于机器学习的方法。我的第一个尝试是使用基于模板匹配的方法。<br />首先，通过观察，人工构建一些高频、重要的模板。但是人工构建模板费时费力，所以决定进行模板挖掘。采用的方法是，对语料预处理后，用 FP-Growth 算法进行频繁项集挖掘，然后再去对挖掘出来的频繁项集进行一些处理，成为候选模板。最后通过机器学习的方法，从候选模板中挑选出认为最靠谱的模板。这些挑选出来的模板可能还需要进一步人工筛选一遍，就得到了新的模板了（但是这会很快，比单纯的直接构造模板快很多）。 |

> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
