时间：2019年7月22日 ~ 2019年7月28日

| 学习人  | 学习任务       | 学习心得和参考资料                                |
| ---- | ---------- | ---------------------------------------- |
| 段金强  | 学习 吴军《信息论》 | 吴军出了很多好书《数学之美》《浪潮之巅》等，内容通俗易懂，从宏观和微观解释的都很明白，以下是截取《信息论》的部分内容，如果有人想要pdf，找我。“ 你可能听说过金融数学这个专业， 那里面的人天天做的事情就是设计这种不容易为人所看懂的， 自己永远不赔钱的金融产品。 而所谓的基金经理， 很多就是把这样的产品卖给你的人。因此， 多了解信息论和基本的数学常识， 可以在生活中省下不少冤枉钱。 这是今天我想告诉你的第二个知识点， 希望你知道， 很多交易和产品都是利用了信息的可度量性， 知道了这点， 就可以看清很多复杂交易背后的原理。掌握了信息量化度量的原理， 你还可以用它来对付当今“信息过载” 的问题， 比如如何判断一篇报道里到底有多少信息量。信息说到底是用于消除不确定性的。 如果讲的事情大部分大家都知道， 信息量就很少。 这也是为什么那些心灵鸡汤的文章大家不愿意读， 并非是它们说的不对， 而是没有信息量。和它们相反的是， 我前面介绍的三篇改变世界的论文， 都非常短， 特别是沃森和克里克的那一篇， 一页纸多一点， 但是把我们过去不知道的 DNA 的结构讲清楚了。 这个信息量就很大。 要点总结：1.香农告诉大家， 信息可以衡量， 但不是用重要性， 而是用信息量， 单位是“比特”。2.你可以把一个充满可能性的系统视为一个“信息源”， 它里面的不确定性叫做“信息熵”， 而“信息” 就是用来消除这些不确定性的， 所以搞清楚黑盒子里是怎么一回事，需要的“信息量” 就等于黑盒子里的“信息熵”。3.很多复杂交易背后其实都用到了信息的可度量性。4.信息量的大小不在于长短， 而在于开创多少新知。" |
| 任星凯  | 总结、学习机器阅读理解 | 最近在读国防科大的ArXiv预印版论文《Neural Machine Reading Comprehension:Methods and Trends》，也是对自己学习了半年左右阅读理解（MRC）的复习与巩固，该论文有39页，给出了机器阅读理解的整体的概括，主要包含三方面：（1）典型MRC任务的定义，区别，表示与数据集；（2）神经网络在MRC上的通用架构，主要模块与流行的方法；（3）新兴的MRC同时其相关的挑战。本周先简单介绍下MRC的基本类型和数据集。**完型填空类型**[CNN/Daily Mail,CBT,LAMBADA,WDW,CLOTH,CLICR],**多选题类型**[MCTest,RACE],**抽取类型**[SQuAD,NewsQA,TriviaQA,DuoRC],**自由回答类型**[bAbI,MS MARCO,NarrativeQA,DuReader]，数据集构建的很多灵感都和咱们从小到大的阅读理解题型一样，让机器给出答案，判断机器是否理解了文本。
|刘洋|长序列多分类比赛|除了继续对TextCNN进行调优（网络参数、seed等）外，还尝试了DPCNN，效果比TextCNN略差，还需要进一步调参。|
|陈俊富|看了一篇动作识别的综述文章|本周看了一篇语义人体动作识别的文章，算是一篇综述文章，主要从语义学，语义空间，以及在语义特征空间中目前一些主流的方法来进行阐述。很值得看，讲解的很细致。年代有点早，[论文地址](https://dl.acm.org/citation.cfm?id=2779412),以后要学会多写论文总结，锻炼一下自己的写作能力。|
| 王耿鑫 | 学习 BERT | 研读了一下 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 论文。其实 BERT 的主要贡献都体现在论文标题上了：其一，这是一个预训练模型，较之前的工作，大大提升了 fine-tuning 的效果，解放了很多需要大量特征工程的特定任务结构；其二，通过 Masked语言模型（MLM），让预训练的深度双向语言表征充分发挥了功力，也因此说明了双向的预训练对语言表示学习的重要性；最后，BERT刷新了11项NLP任务的性能记录，并公布了其代码。|
| Yang | Attention |阅读了 Attention 论文。|

> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
