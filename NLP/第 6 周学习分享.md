时间：2019年4月29日 ~ 2019年5月5日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
段金强 | 学习知识图谱构建流程|  一、构建知识图结构1、读取文件，获得实体，实体之间的关系；2、创建节点，为每个标签（实体），都创建图节点（用neo4j图数据库）；3、创建节点属性，即每个实体的通用属性；4、执行创建实体图的操作；5、执行创建实体和实体之间的关系边；二、抽取问题意图；1、数据处理，把原来实体库放到列表中；2、为每个实体都构造一个ctree；3、把输入的问题，分别用实体tree去匹配，匹配失败，用相似度计算找相近词；4、列常出问问题的关键词集合；5、预测意图（用朴素贝叶斯，TF-IDF进行意图预测）；6、构建问题的可能组合规则，用预测的意图结合匹配到的tree搜索答案，并返回结果
任星凯 | 回顾之前的主流阅读理解模型|  **Match-LSTM(2016提出， 发表于ICLR 17)** Match-LSTM是首个应用于SQuAD数据的端到端机器阅读理解模型，并成功超越原有使用人工特征进行答案抽取的基线模型。该模型的特点是：（1）在文本编码层使用单向LSTM进行语义建模；（2）在语义交互层对支撑文档中的每个词计算该词在问题编码向量上的注意力分配向量，将这一注意力分配向量与问题编码向量点乘获得文档词--问题交互向量，并再拼接上文档词编码向量，最后用一个新的单向LSTM网络对拼接后的向量进行二次语义编码；（3)用反向LSTM重复（1)、（2)操作，并将正反向二次语义编码向量拼接。**BIDAF(2016提出， 发表于ICLR 17)** BIDAF可以视作对Match-LSTM匹配模型的改进。其主要变化在于：（1)词向量化层增加了对字的向量化；（2)在语义交互	充了问题中每个词在文档编码向量上的注意力分配向量， 以提升文档词-问题交互向量的语义交互程度；（3）改用双向LSTM网络二次语义编码。与单向LSTM相比，双向LSTM可以同时提取到每个词相关的上下文信息。**R-Net(发表于ACL 17)** R-Net是对Match-LSTM匹配模型的改进。这一模型最大的特点是采用了双语义交互层设计。在一级语义交互层，R-Net仿照Match-LSTM实现将问题信息融入到每个文档词中去；而在二级语义交互层，R-Net则使用相同办法将已经获得的文档词--问题语义编码向量再度与问题编码向量二次融合，进一步加强语义匹配。**QANet(发表于ICLR 18)** QANet则是一种在BIDAF模型基础上为追求效率而设计的模型。该模型非常创新地在文本编码层使用CNN与Multi-Head Self-Attention机制实现语义编码，由于CNN可以捕捉局部特征、Self-Attention能够捕捉全局特征，因此完全可以用 它们替代传统的LSTM网络。此外，由于CNN的建模效率显著高于LSTM网络，该模型以在更大规模的数据集上进行深度学习——泛化能力得到了进一步提升。这一模型可以在SQuAD数据集上达到训练速度提高3〜13倍！推理速度提高4~9倍，且获得与先前基于LSTM网络媲美的精度。**V-net(百度公司发表于ACL 18)** V-net是一种新的多文档校验深度神经网络建模方法，该模型通过注意力使不同候选文档抽取的答案能够互相印证，从而预测出更好的答案。
李昌群 | 研究Bert代码 | word embedding（构造embedding_table，进行word embedding，可选one_hot的方式，返回embedding的结果和embedding_table）<br />词向量的后续处理（主要是信息添加，可以将word的位置和word对应的token type等信息添加到词向量里面，并且layer正则化和dropout之后返回）<br />构造attention mask（将shape为[batch_size, to_seq_length]的2D mask转换为一个shape 为[batch_size, from_seq_length, to_seq_length] 的3D mask用于attention当中）<br />还在研究 


> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
