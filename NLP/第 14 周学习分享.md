时间：2019年6月24日 ~ 2019年6月30日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
李昌群 | 阅读理解模型论文Gated Attention Reader | 论文针对cloze-style问题提出，迁移到RACE数据集上。可以把其看成是AS reader模型的扩展，尽管模型简单，但是也取得了不错的成绩，也证明了乘法操作机制带来的效果显著。论文提出的Gated Attention用更细粒度的attention计算获得answer的定位，收获了一定的效果，并且使用Multi-Hop的结构，带着问题重读文章K次，增量式地重新得到tokens的表示，可以进一步帮助锁定答案。
刘洋|关系抽取|本周学习了PCNN和split CNN，其中PCNN是对文本分割成三段：实体1左边的部分、实体1到实体2的部分，实体2右边的部分，然后在三段分别进行全局最大池化，将结果合并到一起。split CNN是把文本分成均等的三部分，然后每一部分分别进行卷积，k max pooling，然后再进行合并。尝试了这两种方法，但是效果一般。
 王耿鑫 | 远程监督 | 本周学习了远程监督。对于关系抽取，目前常规的做法之一是将其视为命名实体识别和关系分类的组合。关系分类的做法是：输入一对实体词及其相关特征，预测关系类别。要构建这样的分类器，涉及到两个问题，①关系类别需要已知；②需要训练数据。这时候，如果有一个知识库，那么我们可以得到一些已知的类别。为了避免人工构建训练数据，远程监督被提出了，它基于知识库来构建训练数据。方法很简单，对于知识库中出现实体对，去语料库匹配句子，之后一起生成特征，将已知的实体关系作为所要预测的类别，这样就构造了训练数据。这里有一个很强的假设：如果我们训练语料中的句子所包含的实体对在知识库中有关系的体现，那么我们认为语料库中所有包含相同实体对的句子都表达此关系。所以，远程监督这种“偷懒”的方式其实会引入很多噪声，不过近些年对噪声的过滤也有不少研究。
陈俊富|命名实体识别|本周主要学习了tensorflow，学习如何构建神经网络，以及各个优化器的作用；报名参加命名实体识别评测，主要识别医学实体，实体分为6类，包括手术、检查、检验等，下载代码调试。在运行代码的过程中，出现ResourceExhaustedError错误，目前正在查找具体Bug原因。学习概率论，最近几周考试，所以本周没看论文。
Yang|FastText、TextCNN|阅读微信推送文章 [推荐：常见NLP模型的代码实现（基于TensorFlow和PyTorch）](https://mp.weixin.qq.com/s/1QvOtO2-Me8bCivQ2Qk40Q)，主要看了 FastText 和 TextCNN 的代码和论文。
Xin|Python环境搭建，神经网络实现|最近在尝试转用Python进行神经网络的应用，本周主要搭建了Python环境和尝试用其进行简单神经网络的实现。在经过各种单独Pyhton库和IDE的安装之后，发现还是anaconda最省事。对于使用Python实现神经网络，目前只是简单实现，并未加入一些复杂的内容。在网上看到了一些Python可调用的并行加速功能的函数，很期待熟悉Python之后，其对神经网络并行加速的表现。
任星凯 | 学习BERT WWM中文版 | 近期ERNIE，XLNet等的出现，都是在原始BERT基础上进行完善，这周主要学习了下HFL提出的Pre-Training with Whole Word Masking for Chinese BERT，即中文版的BERT WWM，文章自称为一篇“技术报告”，主要是将原始BERT的mask单字，改为了mask全词，比如分完词后句子为：“使用 语言 模型来 预测下 一个 词 的 probability”，这时如果随机概率要mask语言的“语”字，则就会mask掉与该字相关的整个词，作者称之为“Whole Word Masking”，这一方法看似简单，但是解决了中文一词多字的问题，作者通过在NER,Classification,MRC,NLI等等NLP任务上进行实验，证明此方法优于原始的BERT和百度的ERNIE，并且给出了使用BERT WWM的一些tips，详见[原论文](https://arxiv.org/abs/1906.08101)
段金强 | 学习ELMo | ELMo（Embeddings from Language Models）：是一种新型深度语境化词表征，可对词进行复杂特征(如句法和语义)和词在语言语境中的变化进行建模(即对多义词进行建模)。我们的词向量是深度双向语言模型(biLM)内部状态的函数，在一个大型文本语料库中预训练而成。使用ElMo产生的表征，可以分以下三步:1、产生pre-trained biLM模型。模型由两层bi-LSTM组成，之间用residual connection连接起来。2、在任务语料上(注意是语料，忽略label)fine tuning上一步得到的biLM模型。可以把这一步看为biLM的domain transfer。3、利用ELMo的word embedding来对任务进行训练。通常的做法是把它们作为输入加到已有的模型中，一般能够明显的提高原模型的表现。缺点：lstm是串行机制，训练时间长
> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
