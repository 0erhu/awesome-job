时间：2019年7月8日 ~ 2019年7月14日

| 学习人  | 学习任务      | 学习心得和参考资料                                |
| ---- | --------- | ---------------------------------------- |
| 陈俊富  | 健康宣教小程序   | 本周放假第一周，跟着老师做了个小项目，写了一个健康宣教的微信小程序，主要是起到展示的功能，数据包含教材，还有在医院的网站针对健康教育方面爬取了一些数据，这周做个收尾，下周开始看论文。 |
| 任星凯  | 法研杯阅读理解比赛 | 法研杯阅读理解比赛的第二阶段快结束了，这次比赛我主要使用了BERT作为基础embedding进行构建模型，这一周也在做最后的一些尝试和集成，相关方法暂时不公开，赛后会写好参赛历程分享下。 |
| Yang | CRF       | 参加 2019 达观杯比赛，了解 NER 任务，使用 CRF 模型，了解 CRF 模型细节。 |
| 李昌群  | 并行编程      | 参加暑期学校第二周（本周结束啦)，了解了一些并行编程的知识。           |
| 刘洋   | 关系抽取      | 使用sentence-level的attention的方法，使得F1-score相比之前的PCNN有所提升。 |
| Xin  | 网络爬虫      | 开始网络爬虫的学习，目前仅仅是简单的从网上爬一些公共信息，比如公交地铁的公共信息，并简单处理。并开始阅读Python网络爬虫从入门到实践。 |
| 段金强  | 学习GPT模型   | GPT模型：一种半监督的方式来处理语言理解的任务，2018年OpenAi提出的论文《Improving Language Understanding by Generative Pre-Training》 本文提出一种半监督的方式来处理语言理解的任务。使用非监督的预训练和监督方式的微调。我们的目标是学习一个通用的语言标示，可以经过很小的调整就应用在各种任务中。这个模型的设置不需要目标任务和非标注的数据集在同一个领域。 模型有两个过程。  1、 使用语言模型学习一个深度模型（非监督预训练）：处理非监督文本的普通方法是用语言模型去最大化语言模型的极大似然。文章中使用的是多层Transformer的decoder的语言模型。这个多层的结构应用multi-headed self-attention在处理输入的文本加上位置信息的前馈网络，输出是词的概念分布。和BERT类似。2、   使用相应的监督目标将这些参数调整到目标任务（监督微调fine-tuning）：这个阶段要对前一个阶段模型的参数，根据监督任务进行调整。即利用上面的预训练结果，来辅助本阶段来微调 。优点：1、 循环神经网络所捕捉到的信息较少，而Transformer可以捕捉到更长范围的信息。2、 计算速度比循环神经网络更快，易于并行化 3、实验结果显示Transformer的效果比ELMo和LSTM网络更好。缺点：对于某些类型的任务需要对输入数据的结构作调整。GPT2.0大概看了下，说比原来的增加更高质量的数据源，和增加更多的训练参数，使结果比原来更好。 |
大鹏鹏 | 数据预处理，xgboost | 本周尝试做了一个银联组织的比赛，预测用户的购买和收藏行为，我主要学习了数据预处理的方法（主要是缺失数据的处理以及object类别数据的处理），以及xgboost的基本使用操作|

> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
