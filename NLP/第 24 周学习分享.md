时间：2019年9月2日 ~ 2019年9月8日

| 学习人  | 学习任务          | 学习心得和参考资料                                |
| ---- | ------------- | ---------------------------------------- |
| 段金强  | 学习 ERNIE预训练模型 | 最近在量子位上看到有推荐百度的nlp预训练模型ERNIE，在中文方面效果比BERT要好，查了一些资料。BERT和ERNIE建模过程不一样，BERT是以字为基础，就很难学到以词为基础的语义信息，ERNIE就弥补了这一点，而且ERNIE还用到了很多种类的数据源（百科类，新闻资讯类和论坛对话），尽可能多的学习现实世界的知识，并尽可能的贴近现实世界学习更多的先验知识。整体架构：由两个堆叠的模块构成，1、T-Encoder这部分就是纯粹的bert结构，在该部分模型中主要负责对输入句子（token embedding, segment embedding和positional embedding）进行编码，整个的过程直接参考bert即可；2. K-Encoder，先是实体信息的引入，该论文使用了TransE训练实体向量，再通过多头Attention进行编码（其实可以用更负责一点的训练方法，应该还有一定的提升空间），然后通过实体对齐技术，将实体向量加入到对应实体的第一个token编码后的向量上。（例如姚明是一个实体，在输入时会被分割成姚、明，最后在这部分引入的姚明这个实体的向量会被加入到要这个字经过bert之后的向量上去），由于直接拼接后向量维度会与其他未拼接的向量维度不同，所以作者在这里加入information fusion layer，另外考虑到后面的实体自编码任务，所以这里在融合信息之后，有实体向量加入的部分需要另外多输出一个实体向量。总结来说，ERNIE是一篇思路非常好的论文，提供了一种很好的实体信息引入思路，并且其新提出的预训练方法也给希望将bert这一模型引入关系抽取领域提供了很好的例子。 |
| 大鹏鹏 | 阅读目标检测相关论文 | 阅读不同目标检测算法论文，主要包括Faster R-CNN和SSD，学到了Triplet Loss的用法并且进行相关公式的推导 |

> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
