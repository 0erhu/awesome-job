时间：2019年9月2日 ~ 2019年9月8日

| 学习人  | 学习任务          | 学习心得和参考资料                                |
| ---- | ------------- | ---------------------------------------- |
| 段金强  | 学习 ERNIE预训练模型 | 最近在量子位上看到有推荐百度的nlp预训练模型ERNIE，在中文方面效果比BERT要好，查了一些资料。BERT和ERNIE建模过程不一样，BERT是以字为基础，就很难学到以词为基础的语义信息，ERNIE就弥补了这一点，而且ERNIE还用到了很多种类的数据源（百科类，新闻资讯类和论坛对话），尽可能多的学习现实世界的知识，并尽可能的贴近现实世界学习更多的先验知识。整体架构：由两个堆叠的模块构成，1、T-Encoder这部分就是纯粹的bert结构，在该部分模型中主要负责对输入句子（token embedding, segment embedding和positional embedding）进行编码，整个的过程直接参考bert即可；2. K-Encoder，先是实体信息的引入，该论文使用了TransE训练实体向量，再通过多头Attention进行编码（其实可以用更负责一点的训练方法，应该还有一定的提升空间），然后通过实体对齐技术，将实体向量加入到对应实体的第一个token编码后的向量上。（例如姚明是一个实体，在输入时会被分割成姚、明，最后在这部分引入的姚明这个实体的向量会被加入到要这个字经过bert之后的向量上去），由于直接拼接后向量维度会与其他未拼接的向量维度不同，所以作者在这里加入information fusion layer，另外考虑到后面的实体自编码任务，所以这里在融合信息之后，有实体向量加入的部分需要另外多输出一个实体向量。总结来说，ERNIE是一篇思路非常好的论文，提供了一种很好的实体信息引入思路，并且其新提出的预训练方法也给希望将bert这一模型引入关系抽取领域提供了很好的例子。 |
| 大鹏鹏 | 阅读目标检测相关论文 | 阅读不同目标检测算法论文，主要包括Faster R-CNN和SSD，学到了Triplet Loss的用法并且进行相关公式的推导 |
| 李昌群 | 总结这几周的内容 | 1.使用机器学习模型来判别文章抄袭问题。思想：通过数据训练模型（新华社文章标注为1，其它为0），如果模型的准确度高，那么可以使用模型去进行预测，如果预测结果和真实标签不同（预测标签为1，而标注为0），是否可以说这篇文章是抄袭的。使用了KNN、SVM、RandomForest、LogisticRegression进行了实验<br>2.新闻人物言论自动提取。涉及技术：获得所有表示“说”的意思的单词；使用NER、Dependency Parsing获的谁说了话，说了什么话。<br>3.语义相似度计算比赛完成 |
| Yang | word2vec 负例采样与层次 softmax | 了解 word2vec 的细节，具体到负例采样的实验参数，层次 softmax 的公式推导，了解数学原理，[相关资源链接](https://www.cnblogs.com/peghoty/p/3857839.html)。|
| anniversary | NLP的数学基础、隐马尔科夫模型 | 学习中科院nlp课程-nlp中的数学基础;学习隐马尔科夫模型，及其在分词应用中的实战。[相关资源链接](https://mp.weixin.qq.com/s/RcMtaBdB2zsmZlVfiM11xA) |
| 任星凯 | 看百度ACL19论文《KT-NET》 | 年初自己认识到阅读理解的未来趋势是BERT+知识图谱(引入外部知识)，当时基本还没有这方面的论文出现，谁如果首先占了这个坑，肯定是顶会，果然百度先占了！这篇论文就是结合外部知识促进了BERT在阅读理解方面的发展，模型主要结构：1.给定questions和passages，KT-NET首先抽取潜在相关的KB embeddings 并编码存入知识记忆中；2.一个BERT-Encoding layer，3.将抽取出来相关的KB embedding集成到BERT表示中，4.self-match去融合BERT表示和知识表示；5.输出答案。思路其实不难，但确实具有一定的工作量，百度先做到了，并且在SQuAD 1.1上达到了最好的单模结果，也刷新了ReCoRD数据集。 |
|Xin |言论提取项目进行中|在使⽤维基百科+新闻语料库制作的词向量的基础上，利用搜索树和动态规划，获得出与“说”意思相近的单词。|

> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 张三的学习心得
这是示例，更新之后请将这段删除。
