## 本期内容

**论文阅读**

- IEEE2017：用卷积神经网络做图像几何匹配
- NAACL2018: 多种 Reward 提升文本摘要质量
- Batch Normalization 原论文

**学习心得**

- 大鹏鹏：入门自然语言处理
- 波特：统计学习方法优化问题探索
- 安芯：视网膜启发式学习
- 昨夜星辰：入门seq2seq模型
- 大灵：了解迁移学习
- 上邪：阅读Yann LeCun标志识别论文
- yang：学习高斯混合模型


## 论文阅读

### IEEE2017：用卷积神经网络做图像几何匹配

Convolutional Neural Network Architecture for Geometric Matching，这篇论文是Ignacio Rocco 发表在PAMI上的一篇文章，通过卷积神经网络的方式完成图像间的几何匹配。Ignacio Rocco目前正在巴黎高等教育学院Willow研究组攻读计算机视觉博士学位。

![](https://mmbiz.qpic.cn/mmbiz_png/icmWrEONNM8Vnku4WZxsJBYKdrk8CnMlclIMeB03y4g9mKUibzRlGqNqsQZiaI7fr9LY1CwczPrl6U2dVENQG7tibA/0?wx_fmt=png)

在这篇文章中作者在传统方法的基础上，提出了一种卷积神经网络架构，模仿标准匹配过程。首先，采用有效的可训练的神经网络特征替换标准的本地特征，这样会允许我们去处理匹配图像间外观的大变化；第二，开发了可训练的匹配和变换估计层，以更有效的方式处理噪声和错误匹配，模仿了特征匹配的良好做法。

最终结果是开发出卷积神经网络架构，可以处理较大的外观变化，因此适用于实例级和类别级的匹配问题。

[论文地址](https://arxiv.org/abs/1703.05593)

### NAACL2018: 多种 Reward 提升文本摘要质量

Multi-Reward Reinforced Summarization with Saliency and Entailment，是 NAACL2018 的一篇文章。

![](https://mmbiz.qpic.cn/mmbiz_png/icmWrEONNM8Vnku4WZxsJBYKdrk8CnMlcASOBoEjAd4bFpGHqzAvic506B73bwn9fWwibJMiagH7OAxMFx24MClx9Q/0?wx_fmt=png)

抽象文本摘要是将长文档压缩成简短摘要的任务，生成的摘要需要满足：关键词组的显著性（saliency），逻辑性（directed logical entailment）和非冗余性（non-redundancy）。为了解决这三个问题，作者提出了增强式的学习方法——采取了两种创新的reward function：ROUGESal和Entail，通过结合coveraged baseline，能够取得state-of-the-art的效果。其中，ROUGESal是ROUGE评测的改进版本，给摘要中重要词或者词组更高的reward；Entail对合乎原文档逻辑推理的摘要给予更高的reward。

此外，文章还提出了一种新颖有效的multi-reward方法，指的是在交替的mini-batch中，对多reward进行训练优化，得到了state-of-the-art的效果。

[论文地址](https://www.aclweb.org/anthology/N18-2102.pdf)

### Batch Normalization 原论文

Batch Normalization: Accelerating Deep Network Training by
Reducing Internal Covariate Shift

论文的中心点：围绕着如何降低 internal covariate shift 进行的， 怎么用 batch normalization来解决。

![](https://mmbiz.qpic.cn/mmbiz_png/icmWrEONNM8Vnku4WZxsJBYKdrk8CnMlc9bU1JLqJpTZhXwia0ZRJYONkAxANCCfbk01YmiaCSzeAzbktHS48iaszA/0?wx_fmt=png)

BN的基本思想：因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动。之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（网络层数越深，反向传播的值浮动越大），所以这导致后向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因，而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正太分布而不是正态分布，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。

[论文地址](https://arxiv.org/abs/1502.03167)


## 学习心得


### 大鹏鹏：入门自然语言处理

本周所学如下：

1. 《大话数据结构》第五章串的部分内容，KMP算法有点不太好理解，当年校招面试也被问过
2. 《数学之美》第三章，了解了自然语言处理中概率平滑的方法：对于出现次数超过一定阈值的词，语料库中他们出现的频度就是概率估计；对于出现次数大于零小于阈值的词，使用“古德-图灵”估计之后的相对频度作为概率估计；而对于没有出现的词，用1减去以上两类概率估计可以得到
3. 观看网易云课堂《深度学习：算法到实战》课程第一章，[做的笔记](https://www.cnblogs.com/dapeng-bupt/p/11600932.html)


### 波特：统计学习方法优化问题探索

求解了统计学习方法中7章的习题7.2。在构造求解最优化问题时，联立的方程组无解。由于最小值在极值点或者边界点上求得，需要先求得所有的极小值点与边界上的点，选择使目标函数最小的点为优化问题的解。在方程组无解时（优化问题的解不为极值点），分别令各个变量等于零（即解在边界上），求出剩下的变量，获得多组可能的解；最后使目标函数最小的解为最优解。


### 安芯：视网膜启发式学习

视网膜启发式学习，这是我最近发现比较好的一个paper，原因：基于神经动力学系统和系统工程学研究其实很少，现在顶会都被灌水灌成太平洋了。针对底层研究这个论文是一个非常好的方向。

![](https://mmbiz.qpic.cn/mmbiz_jpg/icmWrEONNM8Vnku4WZxsJBYKdrk8CnMlcwQ1FAUTyGm3oqO9MghibpnUJF0w7QQqXPBtiavZx7kgsqwziaSA1MjRNw/0?wx_fmt=jpeg)

### 昨夜星辰：入门seq2seq模型

本周主要学习了seq2seq模型。传统的RNN有1Vn、nVn、nV1等模式，但有时我们想得到nVm的结果，比如机器翻译，输入“I am three years old”，输出“我三岁”，很明显输入是5个token，输出只有3个token。如果仅仅使用一个RNN模型，很难达到这种效果。因此，可采用encoder-decoder架构，seq2seq可认为是encoder-decoder的一种应用。Seq2seq简单来讲是用一个中间变量c连接左右两个RNN（也可以是RNN的变形，如LSTM、GRU，这里统称为RNN）。左边的RNN即encoder，这里和我们所学的RNN基本相同，即输入“I am three years old（以词向量的形式），最终我们可以输出y，也可以得到最后一层的隐态。[我写的博客笔记](https://blog.csdn.net/weixin_43178406/article/details/102855975)

### 大灵：了解迁移学习

这周主要在学习迁移学习。不过理论知识没怎么学，主要在用pytorch实践。主要是想实现用densenet进行迁移学习。但最后发现densnet确实不好弄。不如resnet易上手，或许这就是为什么用它的人挺少的原因之一吧。不过densenet比较好的一点是，我用他训练出来的模型都挺小的，就只有几百k(没有用预训练模型)，普遍比我用其他网络小。不过得到的结果也稍微奇特一点。。。至少其他网络我得到的形状是圆的，而他是方的。估计是因为他的参数减得太少的缘故。

### 上邪：阅读Yann LeCun标志识别论文

看了一篇Yann LeCun的论文（Traffic Sign Recognition with Multi-Scale Convolutional Networks, 2011），来自德国交通标志牌的识别比赛（GTSRB）。不同于之后的Alex、VGG等大型CNN架构，这个模型架构很轻巧，只有几层，但是效果很好（准确率达到98.97%，超过人类识别率）。

![](https://mmbiz.qpic.cn/mmbiz_jpg/icmWrEONNM8Vnku4WZxsJBYKdrk8CnMlcq54CWgicV3oNb5DYycyRqtnpQS6LXKSqw1uNNcCHicWq817dA2IjQ1lg/0?wx_fmt=jpeg)

### yang：学习高斯混合模型

高斯混合模型（Gaussian Mixed Model）是一种比较常见的聚类方法，相比于 KMeans（硬聚类），GMM 模型是一种软聚类的方式，是概率密度的万能近似器（Universal Approximator），在这种意义下，任何平滑的概率密度都可以用具有足够多组件的高斯混合模型以任意精度来逼近。但是 GMM 并不能通过极大似然的方式得到模型的参数，而需要 EM（期望最大化）的求解最优参数解。高斯混合模型假设每个簇的数据都是符合高斯分布（正态分布）的，数据所呈现的分布就是各个簇的高斯分布按照一定比例叠加后的结果。
使用 EM 算法，假设所有数据的概率分布是由 K（一个超参数）个正态分布加权叠加而成的。GMM 算法有 3K 个参数（一种说法是 3K-1 个，因为是加权求和，权重之和为一，知道 K-1 个权重，剩下的一个权重就确定了）。

E-step 计算每个样本对于每个每个正态分布的响应度，M-step 通过响应度来更新每个正态分布的权重，均值，协方差。

Reference
1. 《统计学习方法》李航
2. 花书《深度学习》Ian Goodfellow / Yoshua Bengio
3. [徐亦达机器学习：Expectation Maximization EM算法](https://www.bilibili.com/video/av23901379?from=search&seid=16776432939996462284)


## 加入我们

公众号内回复「**自学**」，即可加入ML自学者俱乐部社群。可以投稿每周学习心得或者优质学习资料，助力团体共同学习进步。

[上期精彩内容](https://mp.weixin.qq.com/s/qdL3OFZc7_ARI0LNmjK2Sw)


