## 刊首语

> 这里记录ML自学者群体，每周分享优秀的学习心得与资料。由于微信不允许外部链接，需要点击文末的「**阅读原文**」，才能访问文中的链接。



## 本期内容

**论文阅读**

- ACL2018：多语义词向量的编码
- 阿里电商诉讼预测多任务模型
- EMNLP2019: 短文本分类的异质图注意力网络

**学习心得**

- 小强：Encoder-Decoder
- 君君：YOLO 框架学习
- 昨夜星辰：词向量学习
- 君君：研究图像模态转换模型
- 奔腾：科大讯飞比赛总结
- 曲奇：优化理论的多目标规划问题

**疑问解答**

- 特征工程中的归一化有什么作用
- 如何解释准确率、召回率和F值
- 特征抽取、特征选择、变化组合区别


## 论文阅读

### ACL2018：多语义词向量的编码

阅读论文 ACL2018 上的: Probabilistic FastText for Multi-Sense Word Embeddings

![](https://mmbiz.qpic.cn/mmbiz_png/icmWrEONNM8UjRl7p01URDfMnQSSsrzdP0Cm7VDZmp01wh6cHPaNLrq9iaySDEKtXeBsA6BCILXRwLsc9WeSF5Lg/0?wx_fmt=png)

已有方法不能很好的应对稀有词与词汇表没有的词的语义问题，文章提出来一种概率词模型，将高斯混合概率与FastText模型结合，具有灵活的子词结构。

高斯分量可以得到多义词意义上的分离，FastText子词结构获取到更多字符级信息与不确定的信息，从而提供了高质量的语义表达。文章模型在一些词相似度数据集上的表现优于之前的模型，在罕见词数据集和其他语言数据集上也有较好的效果。本文工作首次利用多模态嵌入来解决罕见词的问题。

[论文地址](https://www.aclweb.org/anthology/P18-1001/)

### 阿里电商诉讼预测多任务模型

今天给大家分享一篇阿里使用多任务模型做电商诉讼预测的论文。论文地址：Legal Intelligence for E-commerce。

![](https://mmbiz.qpic.cn/mmbiz_png/icmWrEONNM8UjRl7p01URDfMnQSSsrzdPmUHvDBcibicTnhhXIODfG3PCfCZqIfp8CAjN7GX38gNVW0zYh707je2g/0?wx_fmt=png)

首先介绍一下文章背景，一般来说会先有电商纠纷（一般是在线的，所以此过程也称为ODR，online dispute resolution），如果买卖双方对纠纷处理结果不满意的话一般会通过律法途径将纠纷问题转为诉讼问题。而诉讼需要请律师（高额的费用），同时法官也需要收集各方面的信息（包括买卖方的历史数据及平台聊天记录等），这些都给诉讼处理带来了很大的困难。

那么诉讼预测会面临什么问题呢？

LDJ（legal dispute judgement）任务在NLP领域做得相对较为成熟，但是电商纠纷/诉讼预测与LDJ还是有一定的区别，所以不能直接套用LDJ预测的方法来解决电商纠纷/诉讼预测任务，但是作者表示这两者有一定的相似度。

相对于纠纷数据来说，电商诉讼数据更为稀疏，也就是训练数据少。

总体上来说，作者使用了多任务网络的模型，纠纷数据训练底层表示，使用诉讼数据微调更上层的网络。

这篇文章会介绍好几次，这次先讲个大概吧。

[论文地址](https://dl.acm.org/citation.cfm?id=3331212)

### EMNLP2019: 短文本分类的异质图注意力网络

短文本分类在新闻及微博等领域得到了广泛的应用。但是，目前的文本分类算法主要集中于长文本分类并且无法直接应用于短文本分类。这是由于短文本分类的两个独有挑战：数据的稀疏和歧义和标签数量较少。

![](https://mmbiz.qpic.cn/mmbiz_png/icmWrEONNM8UjRl7p01URDfMnQSSsrzdP7geof7uzmhSD2vACcxg9OO7X9g1GmeQJ51BUpSiaBtVOCtGrQaKxIDQ/0?wx_fmt=png)

针对上述两个挑战，本文创新地将短文本建模为异质图，通过图数据的复杂交互来解决数据稀疏和歧义带来的挑战。同时，本文提出了一种异质图注意力 HGAT 来学习短文本的表示并进行分类。HGAT 是一种半监督学习算法可以更好的适用于标签数量较少的场景，如短文本的分类。

[论文地址](http://www.shichuan.org/doc/74.pdf)

## 学习心得

### 小强：Encoder-Decoder
本周在看 encoder–decoder，seq2seq，attention 相关的东西。

由于时间不够，先简单说几点收获，过几天发成文。

encoder–decoder 是一个很大的范畴，不论编码和解码用的什么算法，只要是『编码–解码』的结构都算这个范畴。

seq2seq 和 encoder–decoder很像。也是编码和解码的过程，不过范围更小，基本限定在了nlp和语音这种序列特征很强的。

如果说encoder–decoder是一种『有损压缩』，在把长文本转化为固定向量的过程会损失一些信息。那么attention就更倾向于无损压缩，更好的保留了信息。

### 君君：YOLO 框架学习

这周看了R-CNN，fast R-CNN，SPP，YOLOv1，YOLOv2的框架结构。觉得YOLO这么简单地模型能有这么高的精确率以及效率是真的厉害。

另外在darknet上跑了下YOLO的预测模型，好奇darknet是怎么用yolo.cfg构造tensorflow模型，就去一行一行看darknet源代码，看得人脑壳疼。看过之后莫名觉得代码能力level+1。

为了高效刷leetcode，找了本算法书籍补补算法代码基础（纯数学系毕业工作才接触的Python+深度学习，觉得硬算法方面太欠缺了），看得比较零散暂时没有总结。

### 昨夜星辰：词向量学习

最近在学词向量的发展史及代码实战。从 one-hot，词袋，tf-idf到分布式表示，如NNLM，以及后来的 word2vec，fasttext，glove，elmo，gpt。

在看到bert时，还是避免不了之前transformer等基础。因此在代码实战上阅读paper原文，掌握原理。希望将当前的基本打牢。

### 君君：研究图像模态转换模型

最近在研究图像模态转换的模型。从改进的condition cycle GAN ，到cycle GAN，再到Wasserstein cycle GAN。
condition cycle GAN被放弃的原因正是因为condition，condition模块的存在使得预测（模态转换）过程必须具备当前图像的label，导致切图转换再还原这个流程走不通。

cycle GAN训练过程中loss_D起伏波动无法很好收敛（尽管大多数类别图像模态转换已经很逼真，但还有少数类别无法完美转换），网上查找之后发现这个原因很有可能是因为GAN本身难收敛的原因，并且为了缓解这种情况已经有人提出了Wasserstein GAN（WGAN）以及WGAN_GP。打算试试Wasserstein cycle GAN这条路能不能走通。

中间为了改进cycle GAN，在github上找了好多基于基础GAN改进的condition GAN、Wasserstein GAN的相关实现代码。参考然后嫁接到自己模型上面，发现GAN的玩法真的是太多了。下面附上两个我觉得总结很到位的GAN链接。有兴趣的可以尝试参考[第一个链接](https://github.com/MASILab/SynSeg-Net)中的变种GAN，在[第二个链接](https://github.com/znxlwm/pytorch-generative-model-collections)中改进cycle GAN。

### 曲奇：优化理论的多目标规划问题

天津大学强化学习实验室同网易伏羲人工智能实验室、NTU 合作的 ASE 2019 论文《Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning》。

该论文主要融合了进化算法与深度强化学习算法，从多目标优化的角度，旨在解决大规模商业游戏的自动化智能测试问题，并荣获 ASE 2019 的最佳论文奖 (Distinguished Paper Award)。


### 奔腾：科大讯飞比赛总结

科大讯飞四个比赛总结
- [AD分类决赛总结](https://blog.csdn.net/herosunly/article/details/102708654)
- [工业寿命预测](https://blog.csdn.net/herosunly/article/details/102711266)
- [大数据应用分类](https://blog.csdn.net/herosunly/article/details/102711596)
- [移动反欺诈](https://blog.csdn.net/herosunly/article/details/102713094)



## 疑问解答

#### [特征工程中的归一化有什么作用](https://www.zhihu.com/question/20455227/answer/325347915)

引入归一化，是由于在不同评价指标中，其量纲或者是量纲单位往往不同，变化区间处于不同的数量级，若不进行归一化，可能导致某些指标被忽视，影响到数据分析的结果。

所以，为了消除特征数据之间的量纲影响，需要进行归一化处理，已解决特征指标之间的可比性。原始数据经过归一化处理后，各指标处于同一数量级，以便进行综合对比评价。

#### [如何解释准确率、召回率和F值](http://bookshadow.com/weblog/2014/06/10/precision-recall-f-measure/)

准确率和召回率是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量。其中精度是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率；召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率。

一般来说，Precision就是检索出来的条目（比如：文档、网页等）有多少是准确的，Recall就是所有准确的条目有多少被检索出来了。

正确率、召回率和 F 值是在鱼龙混杂的环境中，选出目标的重要评价指标。不妨看看这些指标的定义先：

- 正确率 = 提取出的正确信息条数 /  提取出的信息条数   
- 召回率 = 提取出的正确信息条数 /  样本中的信息条数   
 
两者取值在0和1之间，数值越接近1，查准率或查全率就越高。

- F 值  = 正确率 * 召回率 * 2 / (正确率 + 召回率) （F 值即为正确率和召回率的调和平均值）

### [特征抽取、特征选择、变化组合区别](https://www.zhihu.com/question/20716506/answer/45658573)

特征抽取，一般是从原始数据中通过计算得到一些特征，如计算用户的购买力区间，平均每个月的购买次数。
特征选择：假设你抽取了100个特征，通过信息增益、互信息等等指标选择了最重要的30个用于建模；
特征变换：如有个特征是用户的年薪，但是这个年薪的区间太大，我们做了一个函数变换，如log（年薪）。
特征组合：如年龄区间+年薪区间做组合。



## 加入我们

公众号内回复「**自学**」，即可加入ML自学者俱乐部社群。可以投稿每周学习心得或者优质学习资料，助力团体共同学习进步。
