## 本期内容

### 奔腾

这周花时间学习了GBDT和XGBoost，GBDT和XGBoost相比最大的区别在于以下三点差异：
1. XGBoost增加了正则化项，将经验风险最小化转换成了结构风险最小化。
2. GBDT采用的是最优化思想，是通过梯度下降求得损失函数的最小值；而XGBoost采用的是解析的思想，通过泰勒公式展开到二阶近似，然后求得解析解。
3. XGBoost相比GBDT提出了新的特征选择方法。而GBDT还是通过以MSE最小作为切分点的准则。
顺便说下小蓝书的讲解GBDT不严谨的一个地方，使用GBDT是基于最优化的思想，沿着负梯度方向就是极小值点，所以往往要乘以一个学习率。但在小蓝书的公式中并没有学习率的体现。另外小蓝书并没有具体说明叶子节点的取值具体是多少，简单来说如果是MSE作为损失，则叶子节点即为样本的平均值。如果是别的损失函数，就是其他取值，具体可参考原始论文。
GBDT的原始论文为《Greedy Function Approximation: A Gradient Boosting Machine》，下载地址为https://statweb.stanford.edu/~jhf/ftp/trebst.pdf。

### 淡淡风

最近看了一篇强化学习的文章，将监督学习和强化学习结合， 用强化学习来选择不同的模型进行预测，得到最佳预测模型组合。查阅相关文献发现类似内容比较少，欢迎各位大神交流指点，代码应如何实现？
附件为实现框架和论文。https://arxiv.org/abs/1811.01846

### Miracle

一、	数据集制作
1、	将采集到的图片关注部分截图；
2、	使用opencv将截的图处理成相同大小的图片；
3、	将数据集制作成Pytorch接口形式，可用DataLoader装载；
二、	模型训练
1、	创建VGG16模型，Pytorch实现；
2、	设置训练epoches和Batchsize；
3、	开始训练过程；
三、	模型测试与保存
1、	设置模型为Eval模式；
2、	调用数据集，进行模型评估；
3、	评估完毕，保存模型；
4、	当需要用图片来测试模型时，需要将图片裁剪成相同大小，并准换为Tensor类型，直接将Tensor类型输入到模型中即可。

### 黄海广

吴恩达老师的深度学习课程（deeplearning.ai），可以说是深度学习入门的最热门课程，我和志愿者编写了这门课的[笔记](https://mp.weixin.qq.com/s/hxHqHnGykjbyZk25GxZ8cA)，并在 github 开源，为满足手机阅读的需要，我将笔记做成了在线版，可以在手机上打开公众号收藏就能学习。

