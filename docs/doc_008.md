## 本期内容

### 奔腾

这周花时间学习了GBDT和XGBoost，GBDT和XGBoost相比最大的区别在于以下三点差异：
1. XGBoost增加了正则化项，将经验风险最小化转换成了结构风险最小化。
2. GBDT采用的是最优化思想，是通过梯度下降求得损失函数的最小值；而XGBoost采用的是解析的思想，通过泰勒公式展开到二阶近似，然后求得解析解。
3. XGBoost相比GBDT提出了新的特征选择方法。而GBDT还是通过以MSE最小作为切分点的准则。
顺便说下小蓝书的讲解GBDT不严谨的一个地方，使用GBDT是基于最优化的思想，沿着负梯度方向就是极小值点，所以往往要乘以一个学习率。但在小蓝书的公式中并没有学习率的体现。另外小蓝书并没有具体说明叶子节点的取值具体是多少，简单来说如果是MSE作为损失，则叶子节点即为样本的平均值。如果是别的损失函数，就是其他取值，具体可参考原始论文。
GBDT的原始论文为《Greedy Function Approximation: A Gradient Boosting Machine》，下载地址为https://statweb.stanford.edu/~jhf/ftp/trebst.pdf。

### 淡淡风

最近看了一篇强化学习的文章，将监督学习和强化学习结合， 用强化学习来选择不同的模型进行预测，得到最佳预测模型组合。查阅相关文献发现类似内容比较少，欢迎各位大神交流指点，代码应如何实现？
附件为实现框架和论文。https://arxiv.org/abs/1811.01846

### Miracle

一、	数据集制作
1. 将采集到的图片关注部分截图；
2. 使用opencv将截的图处理成相同大小的图片；
3. 将数据集制作成Pytorch接口形式，可用DataLoader装载；

二、	模型训练
1. 创建VGG16模型，Pytorch实现；
2. 设置训练epoches和Batchsize；
3. 开始训练过程；

三、	模型测试与保存
1. 设置模型为Eval模式；
2. 调用数据集，进行模型评估；
3. 评估完毕，保存模型；
4. 当需要用图片来测试模型时，需要将图片裁剪成相同大小，并准换为Tensor类型，直接将Tensor类型输入到模型中即可。

### 黄海广

吴恩达老师的深度学习课程（deeplearning.ai），可以说是深度学习入门的最热门课程，我和志愿者编写了这门课的[笔记](https://mp.weixin.qq.com/s/hxHqHnGykjbyZk25GxZ8cA)，并在 github 开源，为满足手机阅读的需要，我将笔记做成了在线版，可以在手机上打开公众号收藏就能学习。

### 小强

准确率和精确率很容易混淆，这次彻底搞明白了。
另外混淆矩阵对于初学者看着很头大，我也图形化的改造了一下，应该更容易理解了。[准确率、精准率、召回率、F1、ROC曲线、AUC曲线](https://mp.weixin.qq.com/s/YQcGfchQ2dZkMSyxBFMDlg)

### 小门神

Yann LeCun 组织了一个“Using Physical Insights for Machine Learning”研讨会并做了一个演讲，演讲主题是“Energy-Based Self-Supervised Learning ”，PPT相当nice！

### o.o

本周主要总结了两篇关于copy机制的paper。Incorporating Copying Mechanism in Sequence-to-Sequence Learning提出的copyNet使得解码器在解码时能自行决定预测的token是由生成模式还是复制模式而来。其网络结构主要是在Bahdanau attention的基础上进一步增加了copy机制。

该机制对于摘要和对话系统来说，可有效提高基于端到端生成文本的流畅度和准确度，并改善了OOV问题。Get To The Point: Summarization with Pointer-Generator Networks针对现有基于attention的seq2seq模型做生成式文本摘要的缺点：

1、模型可能会错误地生成事实细节；

2、模型可能会重复生成某些内容。

作者们在原有模型基础上提出了两种改进的方案：

1、解码时使用混合的pointer-generator网络结构(思想与copyNet相同，形式不同，相比copyNet，结构更加清晰)。即利用pointer从源文本中copy内容，可有效避免错误事实的生成；利用generator从词表中生成词。pointer-generator结构使模型同时具备了抽取式和生成式摘要的能力；

2、解码输出时增加coverage约束。即在解码时抑制已经在源文本中出现过的内容的attention，可有效消除重复内容的出现。

[详细总结](https://carlos9310.github.io/2019/11/19/add-copy-to-seq2seq-with-attention/)


### 益力多曲奇

推荐一本好书，来自清华大学出版社的:《[智能数据挖掘——面向不确定数据的频繁模式](https://blog.csdn.net/beautiful_well/article/details/103113535)》于晓梅 王红著。总结不确定数据环境下频繁模式挖掘领域的主要研究成果，提供一个好用pattern mining数据挖掘开源软件SPMF。有助于梳理科研思路 

### 昨夜星辰

本周再次回顾了树模型。从决策树到xgboost。决策树主要有ID3 C4.5 CART。基于决策树的弱分类器发展出bagging boosting stacking集成学习，如随机森林，Adaboost，GBDT,XGBOOST,LIGHTGBM。对于决策树做二分类，多分类，回归原理都有所了解，但adaboost,,xgboost,gbdt等如何做多分类仍存在疑问，暂时未仍解决

### Hug 蜗牛

1.最近一直在研究亚马逊开源神经机器翻译框架sockeye，该框架基于Apache 
 mxnet，[Sockeye](https://github.com/awslabs/sockeye)
 
 说明文档：http://sockeye.readthedocs.io/en/latest/ 目前正在看说明文档，以及框架的用法。下周做学习总结分享。
 
2.刷题：LeetCode，刷题是一个快乐的过程，看不懂题目的痛苦与解决问题后的喜悦，悲欢交织，继续坚持。



